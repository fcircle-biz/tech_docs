<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習学習教材 第5章 - 教師あり学習：分類</title>

    <!-- Bootstrap 5 CDN -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Google Fonts - Noto Sans JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Highlight.js CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <!-- Mermaid.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        body {
            font-family: 'Noto Sans JP', sans-serif;
            padding-top: 56px;
        }

        .navbar {
            background-color: #00897B;
        }

        .sidebar {
            position: sticky;
            top: 70px;
            height: calc(100vh - 70px);
            overflow-y: auto;
            background-color: #f8f9fa;
            padding: 1rem;
        }

        .chapter-title {
            color: #00897B;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            border-bottom: 3px solid #00897B;
            padding-bottom: 0.5rem;
        }

        .section-title {
            color: #26a69a;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            padding-left: 0.5rem;
            border-left: 4px solid #26a69a;
        }

        .quiz-container {
            background-color: #e0f2f1;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #00897B;
        }

        .exercise-container {
            background-color: #f3e5f5;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #9c27b0;
        }

        .highlight {
            background-color: #fff9c4;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #fbc02d;
        }

        .warning {
            background-color: #ffebee;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #f44336;
        }

        .code-block {
            background-color: #1e1e1e;
            color: white;
            border-radius: 5px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .nav-link.active {
            background-color: #00897B !important;
            color: white !important;
            border-radius: 5px;
        }

        .nav-link {
            color: #333;
            transition: all 0.3s;
        }

        .nav-link:hover {
            background-color: #e0f2f1;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="README.html">
                <strong>機械学習学習教材</strong>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
    </nav>

    <div class="container-fluid">
        <div class="row">
            <!-- サイドバー -->
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted">
                        <span>学習章</span>
                    </h6>
                    <ul class="nav flex-column">
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-01.html">
                                第1章: 機械学習の概要と環境構築
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-02.html">
                                第2章: 機械学習の基礎概念
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-03.html">
                                第3章: データの前処理
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-04.html">
                                第4章: 教師あり学習：回帰
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="machine-learning-learning-material-05.html">
                                第5章: 教師あり学習：分類
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-06.html">
                                第6章: モデルの評価と検証
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-07.html">
                                第7章: 特徴量エンジニアリング
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-08.html">
                                第8章: アンサンブル学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-09.html">
                                第9章: 教師なし学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-10.html">
                                第10章: 実践プロジェクト開発
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>

            <!-- メインコンテンツ -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2">第5章: 教師あり学習：分類</h1>
                </div>

                <div id="chapter5">
                    <h2 class="chapter-title">分類アルゴリズムの理論と実装</h2>

                    <div class="highlight">
                        <h5>この章で学ぶこと</h5>
                        <ul>
                            <li>分類問題の定義と応用分野を理解する</li>
                            <li>ロジスティック回帰で確率的な分類を行う</li>
                            <li>決定木で解釈しやすいモデルを構築する</li>
                            <li>k近傍法（k-NN）で近似的な分類を行う</li>
                            <li>サポートベクターマシン（SVM）で複雑な境界を学習する</li>
                            <li>分類モデルの性能を適切に評価する</li>
                        </ul>
                    </div>

                    <h3 class="section-title">5.1 分類とは何か</h3>
                    <p>
                        <strong>分類（Classification）</strong>は、入力データをあらかじめ定義されたカテゴリ（クラス）に
                        分けるタスクです。回帰が連続値を予測するのに対し、分類は離散的なラベルを予測します。
                    </p>

                    <h4>分類問題の実例</h4>
                    <ul>
                        <li><strong>メール分類</strong>: スパムか正常メールかを判定</li>
                        <li><strong>医療診断</strong>: 病気の有無や種類を診断</li>
                        <li><strong>画像認識</strong>: 画像に写っている物体を識別</li>
                        <li><strong>信用審査</strong>: ローンの承認/却下を判定</li>
                        <li><strong>顧客分類</strong>: 顧客を購買パターンに基づいてセグメント化</li>
                        <li><strong>感情分析</strong>: テキストがポジティブかネガティブかを判定</li>
                    </ul>

                    <h4>二値分類と多クラス分類</h4>
                    <div class="mermaid">
                        flowchart TD
                            A[分類問題] --> B["二値分類<br/>（Binary Classification）"]
                            A --> C["多クラス分類<br/>（Multi-class Classification）"]
                            B --> D["2つのクラス<br/>例: スパム/正常"]
                            C --> E["3つ以上のクラス<br/>例: 犬/猫/鳥"]
                            style A fill:#00897B,color:#fff
                            style B fill:#4db6ac
                            style C fill:#4db6ac
                    </div>

                    <h3 class="section-title">5.2 ロジスティック回帰</h3>
                    <p>
                        <strong>ロジスティック回帰（Logistic Regression）</strong>は、
                        名前に「回帰」とありますが、実際には分類アルゴリズムです。
                        各クラスに属する確率を出力し、最も確率の高いクラスを予測します。
                    </p>

                    <h4>ロジスティック回帰の仕組み</h4>
                    <ul>
                        <li><strong>線形結合</strong>: 特徴量の重み付き和を計算</li>
                        <li><strong>シグモイド関数</strong>: 線形結合の結果を0～1の確率に変換</li>
                        <li><strong>閾値判定</strong>: 確率が0.5以上ならクラス1、未満ならクラス0</li>
                    </ul>

                    <p>
                        シグモイド関数は、どんな値も0～1の範囲に変換します。
                        これにより、予測結果を確率として解釈できます。
                    </p>

                    <div class="mermaid">
                        flowchart LR
                            A["特徴量<br/>x₁, x₂, ..."] --> B["線形結合<br/>w₁x₁ + w₂x₂ + ..."]
                            B --> C["シグモイド関数<br/>σ(z)"]
                            C --> D["確率<br/>0～1"]
                            D --> E["クラス予測<br/>0 or 1"]
                            style A fill:#e3f2fd
                            style B fill:#c8e6c9
                            style C fill:#c8e6c9
                            style D fill:#fff9c4
                            style E fill:#ffcdd2
                    </div>

                    <div class="exercise-container">
                        <h5>実習 5-1: ロジスティック回帰の実装</h5>
                        <p>アヤメのデータセットを使って二値分類を行います。</p>
                        <h6>手順</h6>
                        <ol>
                            <li>データの読み込みと前処理</li>
                            <li>訓練データとテストデータに分割</li>
                            <li>ロジスティック回帰モデルの訓練</li>
                            <li>予測と確率の確認</li>
                        </ol>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# データ読み込み（2クラスのみ使用）
iris = load_iris()
X = iris.data[iris.target != 2]  # クラス0と1のみ
y = iris.target[iris.target != 2]

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# モデル作成と訓練
model = LogisticRegression()
model.fit(X_train, y_train)

# 予測
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)  # 各クラスの確率

print(f"予測: {y_pred[:5]}")
print(f"確率: {y_proba[:5]}")  # [クラス0の確率, クラス1の確率]</code></pre>
                        <h6>解説</h6>
                        <p>
                            predict_proba()は各クラスに属する確率を返します。
                            例えば[0.2, 0.8]なら、クラス0が20%、クラス1が80%の確率です。
                        </p>
                    </div>

                    <h3 class="section-title">5.3 決定木</h3>
                    <p>
                        <strong>決定木（Decision Tree）</strong>は、
                        if-then-elseルールを階層的に組み合わせて分類を行うアルゴリズムです。
                        人間が理解しやすく、可視化も容易なため、実務でよく使われます。
                    </p>

                    <h4>決定木の仕組み</h4>
                    <ul>
                        <li><strong>ルートノード</strong>: すべてのデータから開始</li>
                        <li><strong>分割</strong>: 最も情報量を増やす特徴量で分割</li>
                        <li><strong>再帰的分割</strong>: 純粋なクラスになるまで繰り返し</li>
                        <li><strong>リーフノード</strong>: 最終的な分類結果</li>
                    </ul>

                    <div class="mermaid">
                        flowchart TD
                            A["花びらの長さ &lt; 2.5cm?"] --> |Yes| B["クラス: Setosa"]
                            A --> |No| C["花びらの幅 &lt; 1.8cm?"]
                            C --> |Yes| D["クラス: Versicolor"]
                            C --> |No| E["クラス: Virginica"]
                            style A fill:#4db6ac
                            style B fill:#81c784
                            style C fill:#4db6ac
                            style D fill:#81c784
                            style E fill:#81c784
                    </div>

                    <h4>決定木の利点と欠点</h4>
                    <table class="table table-bordered">
                        <thead>
                            <tr>
                                <th>利点</th>
                                <th>欠点</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>解釈しやすく、可視化できる</td>
                                <td>過学習しやすい</td>
                            </tr>
                            <tr>
                                <td>データの前処理が少なくて済む</td>
                                <td>不安定（データが少し変わると構造が大きく変わる）</td>
                            </tr>
                            <tr>
                                <td>数値・カテゴリカル両方に対応</td>
                                <td>線形な関係を捉えるのが苦手</td>
                            </tr>
                            <tr>
                                <td>非線形パターンを自然に捉える</td>
                                <td>バイアスが生じやすい</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="exercise-container">
                        <h5>実習 5-2: 決定木の実装と可視化</h5>
                        <p>決定木を構築し、その構造を可視化します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# モデル作成（深さを制限して過学習を防ぐ）
tree_model = DecisionTreeClassifier(max_depth=3, random_state=42)
tree_model.fit(X_train, y_train)

# 予測
y_pred = tree_model.predict(X_test)

# 決定木の可視化
plt.figure(figsize=(12, 8))
plot_tree(tree_model, feature_names=iris.feature_names,
          class_names=iris.target_names[:2], filled=True)
plt.show()

# 特徴量の重要度
importance = tree_model.feature_importances_
for name, imp in zip(iris.feature_names, importance):
    print(f"{name}: {imp:.3f}")</code></pre>
                        <h6>重要なパラメータ</h6>
                        <ul>
                            <li><strong>max_depth</strong>: 木の最大深さ（過学習防止）</li>
                            <li><strong>min_samples_split</strong>: 分割に必要な最小サンプル数</li>
                            <li><strong>min_samples_leaf</strong>: リーフノードの最小サンプル数</li>
                        </ul>
                    </div>

                    <h3 class="section-title">5.4 k近傍法（k-NN）</h3>
                    <p>
                        <strong>k近傍法（k-Nearest Neighbors, k-NN）</strong>は、
                        予測対象に最も近いk個のデータ点を見つけ、それらの多数決でクラスを決定するシンプルなアルゴリズムです。
                    </p>

                    <h4>k-NNの仕組み</h4>
                    <ol>
                        <li>予測したいデータ点と訓練データの距離を計算</li>
                        <li>最も近いk個のデータ点を選択</li>
                        <li>k個のデータ点の中で最も多いクラスを予測結果とする</li>
                    </ol>

                    <div class="mermaid">
                        flowchart TD
                            A["新しいデータ<br/>（予測対象）"] --> B["距離計算<br/>（ユークリッド距離など）"]
                            B --> C["k個の<br/>最近傍を選択"]
                            C --> D["多数決<br/>（最頻クラス）"]
                            D --> E["クラス予測"]
                            style A fill:#e3f2fd
                            style B fill:#c8e6c9
                            style C fill:#c8e6c9
                            style D fill:#fff9c4
                            style E fill:#ffcdd2
                    </div>

                    <h4>kの選び方</h4>
                    <ul>
                        <li><strong>k=1</strong>: 最も近い1点だけで判定。ノイズに敏感で過学習しやすい</li>
                        <li><strong>k=小さい値</strong>: 境界が複雑になり、過学習のリスク</li>
                        <li><strong>k=大きい値</strong>: 境界が滑らかだが、局所的なパターンを見逃す</li>
                        <li><strong>推奨</strong>: 交差検証で最適なkを選ぶ。奇数を推奨（同票を避けるため）</li>
                    </ul>

                    <div class="warning">
                        <h5>注意：スケーリングの重要性</h5>
                        <p>
                            k-NNは距離ベースのアルゴリズムなので、特徴量のスケールが結果に大きく影響します。
                            必ず標準化やMin-Maxスケーリングを適用してください。
                        </p>
                    </div>

                    <div class="exercise-container">
                        <h5>実習 5-3: k-NNの実装</h5>
                        <p>異なるkの値で性能を比較します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# 標準化（k-NNには必須）
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 異なるkの値で試す
for k in [1, 3, 5, 7, 9]:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    score = knn.score(X_test_scaled, y_test)
    print(f"k={k}: 精度 {score:.3f}")</code></pre>
                        <h6>観察ポイント</h6>
                        <p>
                            kが小さいと訓練データに過度にフィットし、大きすぎると単純すぎるモデルになります。
                            最適なkはデータの特性によって異なります。
                        </p>
                    </div>

                    <h3 class="section-title">5.5 サポートベクターマシン（SVM）</h3>
                    <p>
                        <strong>サポートベクターマシン（Support Vector Machine, SVM）</strong>は、
                        クラス間の境界（決定境界）を最大マージンで引くことで、高い汎化性能を実現するアルゴリズムです。
                    </p>

                    <h4>SVMの基本概念</h4>
                    <ul>
                        <li><strong>決定境界</strong>: クラスを分ける境界線（超平面）</li>
                        <li><strong>マージン</strong>: 境界線から最も近いデータ点までの距離</li>
                        <li><strong>サポートベクター</strong>: マージン上にある、決定境界を定義する重要なデータ点</li>
                        <li><strong>最大マージン</strong>: マージンを最大化することで汎化性能を向上</li>
                    </ul>

                    <div class="mermaid">
                        flowchart TD
                            A[SVMの目標] --> B["マージンを<br/>最大化"]
                            B --> C["汎化性能<br/>向上"]
                            A --> D["決定境界を<br/>見つける"]
                            D --> E["クラスを<br/>正確に分離"]
                            style A fill:#00897B,color:#fff
                    </div>

                    <h4>カーネルトリック</h4>
                    <p>
                        線形分離できないデータに対して、<strong>カーネルトリック</strong>を使うことで、
                        データを高次元空間に写像し、そこで線形分離を行います。
                    </p>
                    <ul>
                        <li><strong>線形カーネル</strong>: 線形分離可能なデータ向け</li>
                        <li><strong>RBFカーネル</strong>: 非線形パターンに対応。最もよく使われる</li>
                        <li><strong>多項式カーネル</strong>: 多項式的な関係を捉える</li>
                    </ul>

                    <div class="exercise-container">
                        <h5>実習 5-4: SVMの実装</h5>
                        <p>異なるカーネルで性能を比較します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.svm import SVC

# 線形カーネル
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train_scaled, y_train)
score_linear = svm_linear.score(X_test_scaled, y_test)

# RBFカーネル（非線形）
svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train_scaled, y_train)
score_rbf = svm_rbf.score(X_test_scaled, y_test)

print(f"線形カーネル精度: {score_linear:.3f}")
print(f"RBFカーネル精度: {score_rbf:.3f}")</code></pre>
                        <h6>重要なパラメータ</h6>
                        <ul>
                            <li><strong>C</strong>: 誤分類のペナルティ。大きいと過学習のリスク</li>
                            <li><strong>gamma</strong>: RBFカーネルの広がり。大きいと過学習しやすい</li>
                        </ul>
                    </div>

                    <h3 class="section-title">5.6 分類モデルの評価指標</h3>
                    <p>
                        分類問題では、単純な精度だけでなく、複数の評価指標を使って
                        モデルの性能を多角的に評価することが重要です。
                    </p>

                    <h4>混同行列（Confusion Matrix）</h4>
                    <p>
                        予測結果と実際のラベルの組み合わせを表にしたもの。
                        すべての評価指標の基礎となります。
                    </p>

                    <table class="table table-bordered text-center">
                        <thead>
                            <tr>
                                <th colspan="2" rowspan="2"></th>
                                <th colspan="2">予測</th>
                            </tr>
                            <tr>
                                <th>陽性</th>
                                <th>陰性</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th rowspan="2">実際</th>
                                <th>陽性</th>
                                <td style="background-color: #c8e6c9;">真陽性（TP）</td>
                                <td style="background-color: #ffcdd2;">偽陰性（FN）</td>
                            </tr>
                            <tr>
                                <th>陰性</th>
                                <td style="background-color: #ffcdd2;">偽陽性（FP）</td>
                                <td style="background-color: #c8e6c9;">真陰性（TN）</td>
                            </tr>
                        </tbody>
                    </table>

                    <h4>主要な評価指標</h4>

                    <h5>1. 精度（Accuracy）</h5>
                    <p>
                        <strong>式</strong>: (TP + TN) / 全体<br>
                        <strong>意味</strong>: 全体の中で正しく予測できた割合<br>
                        <strong>注意</strong>: クラスのバランスが偏っている場合は誤解を招く
                    </p>

                    <h5>2. 適合率（Precision）</h5>
                    <p>
                        <strong>式</strong>: TP / (TP + FP)<br>
                        <strong>意味</strong>: 陽性と予測したものの中で、実際に陽性だった割合<br>
                        <strong>重視する場合</strong>: 偽陽性のコストが高い（例：スパムフィルタ）
                    </p>

                    <h5>3. 再現率（Recall / Sensitivity）</h5>
                    <p>
                        <strong>式</strong>: TP / (TP + FN)<br>
                        <strong>意味</strong>: 実際の陽性のうち、正しく陽性と予測できた割合<br>
                        <strong>重視する場合</strong>: 偽陰性のコストが高い（例：病気の診断）
                    </p>

                    <h5>4. F1スコア</h5>
                    <p>
                        <strong>式</strong>: 2 × (Precision × Recall) / (Precision + Recall)<br>
                        <strong>意味</strong>: 適合率と再現率の調和平均<br>
                        <strong>特徴</strong>: バランスの取れた総合評価指標
                    </p>

                    <div class="mermaid">
                        flowchart TD
                            A[混同行列] --> B[精度<br/>Accuracy]
                            A --> C[適合率<br/>Precision]
                            A --> D[再現率<br/>Recall]
                            C --> E["F1スコア<br/>（調和平均）"]
                            D --> E
                            style A fill:#00897B,color:#fff
                    </div>

                    <div class="exercise-container">
                        <h5>実習 5-5: 評価指標の計算</h5>
                        <p>様々な評価指標を使ってモデルを評価します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report
)

# 予測
y_pred = model.predict(X_test)

# 各種指標の計算
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"精度: {accuracy:.3f}")
print(f"適合率: {precision:.3f}")
print(f"再現率: {recall:.3f}")
print(f"F1スコア: {f1:.3f}")

# 混同行列
cm = confusion_matrix(y_test, y_pred)
print("\n混同行列:")
print(cm)

# 詳細レポート
print("\n分類レポート:")
print(classification_report(y_test, y_pred))</code></pre>
                        <h6>結果の解釈</h6>
                        <p>
                            classification_reportは、各クラスごとの精度、再現率、F1スコアを表示します。
                            クラスごとのパフォーマンスの違いを確認し、改善点を見つけましょう。
                        </p>
                    </div>

                    <h3 class="section-title">5.7 分類アルゴリズムの使い分け</h3>
                    <p>
                        問題の特性やデータの性質に応じて、適切なアルゴリズムを選択することが重要です。
                    </p>

                    <table class="table table-bordered">
                        <thead>
                            <tr>
                                <th>アルゴリズム</th>
                                <th>適する状況</th>
                                <th>注意点</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>ロジスティック回帰</td>
                                <td>線形分離可能、確率が必要</td>
                                <td>非線形パターンには弱い</td>
                            </tr>
                            <tr>
                                <td>決定木</td>
                                <td>解釈性が重要、非線形パターン</td>
                                <td>過学習しやすい</td>
                            </tr>
                            <tr>
                                <td>k-NN</td>
                                <td>局所的なパターン、少量データ</td>
                                <td>大規模データでは遅い</td>
                            </tr>
                            <tr>
                                <td>SVM</td>
                                <td>高次元データ、複雑な境界</td>
                                <td>大規模データには不向き</td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="quiz-container">
                        <h5>理解度確認クイズ</h5>
                        <ol>
                            <li>
                                <strong>分類と回帰の違い</strong><br>
                                分類問題と回帰問題の主な違いは何ですか？それぞれの応用例を挙げてください。
                            </li>
                            <li>
                                <strong>ロジスティック回帰</strong><br>
                                ロジスティック回帰がシグモイド関数を使う理由は何ですか？
                            </li>
                            <li>
                                <strong>決定木の深さ</strong><br>
                                決定木のmax_depthパラメータを大きくしすぎるとどうなりますか？
                            </li>
                            <li>
                                <strong>k-NNのkの選択</strong><br>
                                k=1とk=100では、それぞれどのような特徴がありますか？
                            </li>
                            <li>
                                <strong>SVMのカーネル</strong><br>
                                RBFカーネルはどのような場合に有効ですか？
                            </li>
                            <li>
                                <strong>評価指標の選択</strong><br>
                                病気の診断システムでは、精度、適合率、再現率のうち、どれを最も重視すべきですか？理由も説明してください。
                            </li>
                        </ol>
                    </div>

                    <h3 class="section-title">5.8 まとめ</h3>
                    <div class="highlight">
                        <h5>本章で学んだこと</h5>
                        <ul>
                            <li>分類はカテゴリを予測する教師あり学習</li>
                            <li>ロジスティック回帰は確率的な分類を提供</li>
                            <li>決定木は解釈しやすく、非線形パターンに対応</li>
                            <li>k-NNは局所的な近傍情報を使った分類</li>
                            <li>SVMはマージンを最大化し、高い汎化性能を実現</li>
                            <li>精度、適合率、再現率、F1スコアで多角的に評価</li>
                            <li>問題に応じて適切なアルゴリズムを選択する</li>
                        </ul>
                    </div>

                    <div class="d-flex justify-content-between mt-4">
                        <a href="machine-learning-learning-material-04.html" class="btn btn-secondary">← 前の章</a>
                        <a href="machine-learning-learning-material-06.html" class="btn btn-primary">次の章 →</a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>
本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
