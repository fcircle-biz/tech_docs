<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習学習教材 第9章 - 教師なし学習</title>

    <!-- Bootstrap 5 CDN -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Google Fonts - Noto Sans JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Highlight.js CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <!-- Mermaid.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        body {
            font-family: 'Noto Sans JP', sans-serif;
            padding-top: 56px;
        }

        .navbar {
            background-color: #00897B;
        }

        .sidebar {
            position: sticky;
            top: 70px;
            height: calc(100vh - 70px);
            overflow-y: auto;
            background-color: #f8f9fa;
            padding: 1rem;
        }

        .chapter-title {
            color: #00897B;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            border-bottom: 3px solid #00897B;
            padding-bottom: 0.5rem;
        }

        .section-title {
            color: #26a69a;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            padding-left: 0.5rem;
            border-left: 4px solid #26a69a;
        }

        .quiz-container {
            background-color: #e0f2f1;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #00897B;
        }

        .exercise-container {
            background-color: #f3e5f5;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #9c27b0;
        }

        .highlight {
            background-color: #fff9c4;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #fbc02d;
        }

        .warning {
            background-color: #ffebee;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #f44336;
        }

        .code-block {
            background-color: #1e1e1e;
            color: white;
            border-radius: 5px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .nav-link.active {
            background-color: #00897B !important;
            color: white !important;
            border-radius: 5px;
        }

        .nav-link {
            color: #333;
            transition: all 0.3s;
        }

        .nav-link:hover {
            background-color: #e0f2f1;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="README.html">
                <strong>機械学習学習教材</strong>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
    </nav>

    <div class="container-fluid">
        <div class="row">
            <!-- サイドバー -->
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted">
                        <span>学習章</span>
                    </h6>
                    <ul class="nav flex-column">
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-01.html">
                                第1章: 機械学習の概要と環境構築
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-02.html">
                                第2章: 機械学習の基礎概念
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-03.html">
                                第3章: データの前処理
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-04.html">
                                第4章: 教師あり学習：回帰
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-05.html">
                                第5章: 教師あり学習：分類
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-06.html">
                                第6章: モデルの評価と検証
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-07.html">
                                第7章: 特徴量エンジニアリング
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-08.html">
                                第8章: アンサンブル学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="machine-learning-learning-material-09.html">
                                第9章: 教師なし学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-10.html">
                                第10章: 実践プロジェクト開発
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>

            <!-- メインコンテンツ -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2">第9章: 教師なし学習</h1>
                </div>

                <div id="chapter9">
                    <h2 class="chapter-title">ラベルなしデータからパターンを発見</h2>

                    <div class="highlight">
                        <h5>この章で学ぶこと</h5>
                        <ul>
                            <li>教師なし学習の目的と応用分野を理解する</li>
                            <li>クラスタリング（k-means、階層的、DBSCAN）の実装</li>
                            <li>次元削減の高度な手法を学ぶ</li>
                            <li>異常検知の基礎を習得する</li>
                            <li>アソシエーション分析でパターンを発見する</li>
                        </ul>
                    </div>

                    <h3 class="section-title">9.1 教師なし学習とは</h3>
                    <p>
                        <strong>教師なし学習（Unsupervised Learning）</strong>は、
                        正解ラベルのないデータから、隠れた構造やパターンを発見する学習手法です。
                        データ探索やグループ分け、異常検知など、幅広い応用があります。
                    </p>

                    <h4>教師なし学習の主なタスク</h4>
                    <ul>
                        <li><strong>クラスタリング</strong>: 類似したデータをグループ化</li>
                        <li><strong>次元削減</strong>: 高次元データを低次元に圧縮</li>
                        <li><strong>異常検知</strong>: 通常と異なるデータを発見</li>
                        <li><strong>アソシエーション分析</strong>: データ間の関連性を発見</li>
                    </ul>

                    <div class="mermaid">
                        flowchart TD
                            A[教師なし学習] --> B["クラスタリング<br/>（グループ分け）"]
                            A --> C["次元削減<br/>（圧縮・可視化）"]
                            A --> D["異常検知<br/>（外れ値発見）"]
                            A --> E["アソシエーション分析<br/>（関連性発見）"]
                            style A fill:#00897B,color:#fff
                    </div>

                    <h3 class="section-title">9.2 クラスタリング</h3>
                    <p>
                        <strong>クラスタリング（Clustering）</strong>は、
                        類似したデータ点を自動的にグループ（クラスタ）にまとめる手法です。
                        事前に正解ラベルは与えられず、データの自然な構造を発見します。
                    </p>

                    <h4>クラスタリングの応用例</h4>
                    <ul>
                        <li><strong>顧客セグメンテーション</strong>: 購買行動で顧客をグループ分け</li>
                        <li><strong>画像圧縮</strong>: 色を代表的な色にまとめる</li>
                        <li><strong>遺伝子解析</strong>: 似た遺伝子パターンをグループ化</li>
                        <li><strong>文書分類</strong>: 記事を自動でトピック別に分類</li>
                    </ul>

                    <h4>k-means法</h4>
                    <p>
                        <strong>k-means</strong>は、最も基本的で広く使われるクラスタリング手法です。
                        k個のクラスタ中心を設定し、各データ点を最も近い中心に割り当てます。
                    </p>

                    <h5>k-meansのアルゴリズム</h5>
                    <ol>
                        <li>ランダムにk個の中心点を初期化</li>
                        <li>各データ点を最も近い中心に割り当て</li>
                        <li>各クラスタの中心を再計算（クラスタ内の平均点）</li>
                        <li>中心が変化しなくなるまで2～3を繰り返す</li>
                    </ol>

                    <div class="mermaid">
                        flowchart LR
                            A["初期化<br/>k個の中心"] --> B["割り当て<br/>最近傍中心"]
                            B --> C["更新<br/>中心を再計算"]
                            C --> D{収束?}
                            D -->|No| B
                            D -->|Yes| E["完了"]
                            style A fill:#e3f2fd
                            style E fill:#c8e6c9
                    </div>

                    <div class="exercise-container">
                        <h5>実習 9-1: k-meansクラスタリングの実装</h5>
                        <p>顧客データをk-meansでグループ化します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# k-meansモデル（3クラスタ）
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X)

# クラスタ中心
centers = kmeans.cluster_centers_

# 可視化
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1],
            c='red', marker='X', s=200, label='中心')
plt.xlabel('特徴量1')
plt.ylabel('特徴量2')
plt.title('k-meansクラスタリング')
plt.legend()
plt.show()

print(f"各クラスタのサイズ: {np.bincount(clusters)}")</code></pre>
                        <h6>クラスタ数の決め方</h6>
                        <p>
                            <strong>エルボー法</strong>: クラスタ数を変えながらイナーシャ（クラスタ内誤差平方和）をプロットし、
                            急激に改善が鈍化する「肘」の位置を選ぶ。
                        </p>
                    </div>

                    <div class="exercise-container">
                        <h5>実習 9-2: エルボー法でクラスタ数を決定</h5>
                        <p>最適なクラスタ数を見つけます。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python"># 異なるクラスタ数で試す
inertias = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertias.append(kmeans.inertia_)

# エルボープロット
plt.plot(K_range, inertias, 'bo-')
plt.xlabel('クラスタ数 k')
plt.ylabel('イナーシャ')
plt.title('エルボー法')
plt.grid(True)
plt.show()</code></pre>
                        <h6>解釈</h6>
                        <p>
                            グラフが急に平坦になる点（肘のような形）が最適なクラスタ数の目安です。
                        </p>
                    </div>

                    <h4>階層的クラスタリング</h4>
                    <p>
                        <strong>階層的クラスタリング</strong>は、
                        データ点を段階的に結合または分割してクラスタの階層構造（デンドログラム）を作ります。
                    </p>

                    <h5>凝集型階層クラスタリング</h5>
                    <ol>
                        <li>各データ点を1つのクラスタとして開始</li>
                        <li>最も近い2つのクラスタを結合</li>
                        <li>すべてが1つになるまで繰り返す</li>
                        <li>適切な高さで切断してクラスタを決定</li>
                    </ol>

                    <div class="exercise-container">
                        <h5>実習 9-3: 階層的クラスタリングの実装</h5>
                        <p>デンドログラムで階層構造を可視化します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# 階層的クラスタリング
hierarchical = AgglomerativeClustering(n_clusters=3)
clusters = hierarchical.fit_predict(X)

# デンドログラムの作成
linkage_matrix = linkage(X, method='ward')
plt.figure(figsize=(10, 5))
dendrogram(linkage_matrix)
plt.xlabel('データ点')
plt.ylabel('距離')
plt.title('デンドログラム')
plt.show()</code></pre>
                        <h6>利点</h6>
                        <ul>
                            <li>クラスタ数を事前に決めなくてよい</li>
                            <li>階層構造を可視化できる</li>
                            <li>小規模データに適している</li>
                        </ul>
                    </div>

                    <h4>DBSCAN（密度ベースクラスタリング）</h4>
                    <p>
                        <strong>DBSCAN（Density-Based Spatial Clustering）</strong>は、
                        データの密度に基づいてクラスタを形成します。
                        k-meansと異なり、任意の形状のクラスタを検出でき、ノイズ（外れ値）も識別できます。
                    </p>

                    <h5>DBSCANの特徴</h5>
                    <ul>
                        <li>クラスタ数を事前に指定不要</li>
                        <li>任意の形状のクラスタを検出</li>
                        <li>外れ値を自動的に識別</li>
                        <li>密度が異なるクラスタの検出が難しい</li>
                    </ul>

                    <div class="exercise-container">
                        <h5>実習 9-4: DBSCANの実装</h5>
                        <p>密度ベースでクラスタを検出します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.cluster import DBSCAN

# DBSCANモデル
dbscan = DBSCAN(eps=0.5, min_samples=5)
clusters = dbscan.fit_predict(X)

# ノイズ点（ラベル-1）の数
n_noise = list(clusters).count(-1)
n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)

print(f"クラスタ数: {n_clusters}")
print(f"ノイズ点数: {n_noise}")

# 可視化
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis')
plt.title('DBSCANクラスタリング')
plt.show()</code></pre>
                        <h6>パラメータ</h6>
                        <ul>
                            <li><strong>eps</strong>: 近傍の半径。小さいと細かいクラスタ</li>
                            <li><strong>min_samples</strong>: コアポイントに必要な最小点数</li>
                        </ul>
                    </div>

                    <h3 class="section-title">9.3 次元削減（詳細）</h3>
                    <p>
                        第7章で学んだPCAとt-SNEに加え、その他の次元削減手法を学びます。
                    </p>

                    <h4>PCAの数学的理解</h4>
                    <p>
                        PCAは、データの分散が最大となる方向（主成分）を見つける線形変換です。
                        固有値分解または特異値分解（SVD）を使って計算されます。
                    </p>

                    <h5>主成分の解釈</h5>
                    <ul>
                        <li><strong>第1主成分</strong>: データの分散が最大の方向</li>
                        <li><strong>第2主成分</strong>: 第1と直交し、残りの分散が最大の方向</li>
                        <li><strong>寄与率</strong>: 各主成分が説明する分散の割合</li>
                    </ul>

                    <h4>カーネルPCA</h4>
                    <p>
                        <strong>カーネルPCA</strong>は、非線形な変換を適用してから主成分分析を行います。
                        線形PCAでは捉えられない複雑なパターンを抽出できます。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 9-5: カーネルPCAの実装</h5>
                        <p>非線形な次元削減を行います。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.decomposition import KernelPCA

# RBFカーネルPCA
kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)
X_kpca = kpca.fit_transform(X)

# 可視化
plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='viridis')
plt.xlabel('第1主成分')
plt.ylabel('第2主成分')
plt.title('カーネルPCA')
plt.colorbar(label='クラス')
plt.show()</code></pre>
                    </div>

                    <h3 class="section-title">9.4 異常検知</h3>
                    <p>
                        <strong>異常検知（Anomaly Detection）</strong>は、
                        通常のデータパターンから外れた異常なデータを検出する技術です。
                        不正検知、故障予知、品質管理などに応用されます。
                    </p>

                    <h4>異常検知の応用例</h4>
                    <ul>
                        <li><strong>不正検知</strong>: クレジットカードの不正使用</li>
                        <li><strong>故障予知</strong>: 機械の異常な動作パターン</li>
                        <li><strong>セキュリティ</strong>: ネットワーク侵入検知</li>
                        <li><strong>品質管理</strong>: 製造プロセスの異常</li>
                    </ul>

                    <h4>Isolation Forest</h4>
                    <p>
                        <strong>Isolation Forest</strong>は、
                        異常値は少数で孤立しているという性質を利用した手法です。
                        ランダムに分割を繰り返し、早く孤立する点を異常とみなします。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 9-6: Isolation Forestによる異常検知</h5>
                        <p>データから異常値を検出します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import IsolationForest

# Isolation Forestモデル
iso_forest = IsolationForest(contamination=0.1, random_state=42)
predictions = iso_forest.fit_predict(X)

# 正常=1, 異常=-1
n_outliers = list(predictions).count(-1)
print(f"検出された異常値: {n_outliers}")

# 可視化
plt.scatter(X[predictions == 1, 0], X[predictions == 1, 1],
            label='正常', alpha=0.6)
plt.scatter(X[predictions == -1, 0], X[predictions == -1, 1],
            c='red', label='異常', marker='x', s=100)
plt.xlabel('特徴量1')
plt.ylabel('特徴量2')
plt.title('Isolation Forest異常検知')
plt.legend()
plt.show()</code></pre>
                        <h6>パラメータ</h6>
                        <p>
                            <strong>contamination</strong>: データ中の異常値の割合（0.1 = 10%）。
                            事前に異常値の割合がわかっている場合に設定します。
                        </p>
                    </div>

                    <h4>One-Class SVM</h4>
                    <p>
                        <strong>One-Class SVM</strong>は、
                        正常データのみで訓練し、正常データを囲む境界を学習します。
                        境界外のデータを異常とみなします。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 9-7: One-Class SVMの実装</h5>
                        <p>正常データの境界を学習します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.svm import OneClassSVM

# One-Class SVMモデル
oc_svm = OneClassSVM(nu=0.1, kernel='rbf', gamma='auto')
oc_svm.fit(X)
predictions = oc_svm.predict(X)

# 結果表示
n_outliers = list(predictions).count(-1)
print(f"検出された異常値: {n_outliers}")</code></pre>
                        <h6>パラメータ</h6>
                        <p>
                            <strong>nu</strong>: 異常値の上限割合。contamination同様、事前知識を反映。
                        </p>
                    </div>

                    <h3 class="section-title">9.5 アソシエーション分析</h3>
                    <p>
                        <strong>アソシエーション分析</strong>は、
                        データ間の関連性や共起パターンを発見する手法です。
                        「AとBを一緒に購入する顧客が多い」といったルールを見つけます。
                    </p>

                    <h4>応用例</h4>
                    <ul>
                        <li><strong>マーケットバスケット分析</strong>: 一緒に購入される商品の発見</li>
                        <li><strong>推薦システム</strong>: 関連商品の提案</li>
                        <li><strong>Webページ分析</strong>: 一緒に訪問されるページの発見</li>
                    </ul>

                    <h4>主要な指標</h4>
                    <ul>
                        <li><strong>サポート</strong>: ルールが出現する頻度</li>
                        <li><strong>信頼度</strong>: Aが起きたときBが起きる確率</li>
                        <li><strong>リフト</strong>: ルールの関連性の強さ（1より大きいと正の相関）</li>
                    </ul>

                    <p>
                        <strong>Aprioriアルゴリズム</strong>が最も有名で、
                        効率的に頻出パターンを発見します。
                    </p>

                    <div class="warning">
                        <h5>注意：大規模データでの計算コスト</h5>
                        <p>
                            アイテム数が多いと組み合わせが爆発的に増加します。
                            最小サポート値を適切に設定して、計算量を抑える必要があります。
                        </p>
                    </div>

                    <div class="quiz-container">
                        <h5>理解度確認クイズ</h5>
                        <ol>
                            <li>
                                <strong>教師あり学習と教師なし学習の違い</strong><br>
                                両者の主な違いは何ですか？それぞれどのような問題に適していますか？
                            </li>
                            <li>
                                <strong>k-meansの初期化</strong><br>
                                k-meansで初期の中心点の位置がなぜ結果に影響しますか？
                            </li>
                            <li>
                                <strong>DBSCANの利点</strong><br>
                                DBSCANがk-meansより優れている点は何ですか？
                            </li>
                            <li>
                                <strong>PCAの寄与率</strong><br>
                                第1主成分の寄与率が60%の場合、どういう意味ですか？
                            </li>
                            <li>
                                <strong>異常検知の評価</strong><br>
                                正解ラベルがない場合、異常検知モデルをどのように評価しますか？
                            </li>
                            <li>
                                <strong>アソシエーションルールのリフト</strong><br>
                                リフト値が2.5のルールは、どのように解釈すればよいですか？
                            </li>
                        </ol>
                    </div>

                    <h3 class="section-title">9.6 まとめ</h3>
                    <div class="highlight">
                        <h5>本章で学んだこと</h5>
                        <ul>
                            <li>教師なし学習はラベルなしデータからパターンを発見</li>
                            <li>k-means、階層的、DBSCANでデータをクラスタリング</li>
                            <li>エルボー法で最適なクラスタ数を決定</li>
                            <li>PCA、カーネルPCA、t-SNEで次元削減</li>
                            <li>Isolation ForestとOne-Class SVMで異常検知</li>
                            <li>アソシエーション分析でデータ間の関連性を発見</li>
                            <li>教師なし学習はデータ探索の強力なツール</li>
                        </ul>
                    </div>

                    <div class="d-flex justify-content-between mt-4">
                        <a href="machine-learning-learning-material-08.html" class="btn btn-secondary">← 前の章</a>
                        <a href="machine-learning-learning-material-10.html" class="btn btn-primary">次の章 →</a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>
本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
