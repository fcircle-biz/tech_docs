<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習学習教材 第8章 - アンサンブル学習</title>

    <!-- Bootstrap 5 CDN -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Google Fonts - Noto Sans JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Highlight.js CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <!-- Mermaid.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        body {
            font-family: 'Noto Sans JP', sans-serif;
            padding-top: 56px;
        }

        .navbar {
            background-color: #00897B;
        }

        .sidebar {
            position: sticky;
            top: 70px;
            height: calc(100vh - 70px);
            overflow-y: auto;
            background-color: #f8f9fa;
            padding: 1rem;
        }

        .chapter-title {
            color: #00897B;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            border-bottom: 3px solid #00897B;
            padding-bottom: 0.5rem;
        }

        .section-title {
            color: #26a69a;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            padding-left: 0.5rem;
            border-left: 4px solid #26a69a;
        }

        .quiz-container {
            background-color: #e0f2f1;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #00897B;
        }

        .exercise-container {
            background-color: #f3e5f5;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #9c27b0;
        }

        .highlight {
            background-color: #fff9c4;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #fbc02d;
        }

        .warning {
            background-color: #ffebee;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #f44336;
        }

        .code-block {
            background-color: #1e1e1e;
            color: white;
            border-radius: 5px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .nav-link.active {
            background-color: #00897B !important;
            color: white !important;
            border-radius: 5px;
        }

        .nav-link {
            color: #333;
            transition: all 0.3s;
        }

        .nav-link:hover {
            background-color: #e0f2f1;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="README.html">
                <strong>機械学習学習教材</strong>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
    </nav>

    <div class="container-fluid">
        <div class="row">
            <!-- サイドバー -->
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted">
                        <span>学習章</span>
                    </h6>
                    <ul class="nav flex-column">
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-1.html">
                                第1章: 機械学習の概要と環境構築
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-2.html">
                                第2章: 機械学習の基礎概念
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-3.html">
                                第3章: データの前処理
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-4.html">
                                第4章: 教師あり学習：回帰
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-5.html">
                                第5章: 教師あり学習：分類
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-6.html">
                                第6章: モデルの評価と検証
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-7.html">
                                第7章: 特徴量エンジニアリング
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="machine-learning-learning-material-8.html">
                                第8章: アンサンブル学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-9.html">
                                第9章: 教師なし学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-10.html">
                                第10章: 実践プロジェクト開発
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>

            <!-- メインコンテンツ -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2">第8章: アンサンブル学習</h1>
                </div>

                <div id="chapter8">
                    <h2 class="chapter-title">複数のモデルを組み合わせて高精度を実現</h2>

                    <div class="highlight">
                        <h5>この章で学ぶこと</h5>
                        <ul>
                            <li>アンサンブル学習の基本概念と利点を理解する</li>
                            <li>バギング（ランダムフォレスト）の仕組みを学ぶ</li>
                            <li>ブースティング（AdaBoost、Gradient Boosting）を習得する</li>
                            <li>XGBoostとLightGBMの実践的な使い方を身につける</li>
                            <li>スタッキングで複数モデルを統合する</li>
                        </ul>
                    </div>

                    <h3 class="section-title">8.1 アンサンブル学習とは</h3>
                    <p>
                        <strong>アンサンブル学習（Ensemble Learning）</strong>は、
                        複数の学習モデルを組み合わせることで、単一モデルより高い予測性能を実現する手法です。
                        「三人寄れば文殊の知恵」という諺のように、複数のモデルの知恵を集めることで精度が向上します。
                    </p>

                    <h4>アンサンブル学習の利点</h4>
                    <ul>
                        <li><strong>精度向上</strong>: 個々のモデルの弱点を補い合い、総合的な精度が上がる</li>
                        <li><strong>過学習の抑制</strong>: 複数モデルの平均を取ることで汎化性能が向上</li>
                        <li><strong>安定性</strong>: データの変動に対してロバスト（頑健）</li>
                        <li><strong>実用性</strong>: Kaggleなど機械学習コンペで上位手法の多くがアンサンブル</li>
                    </ul>

                    <div class="mermaid">
                        flowchart TD
                            A[アンサンブル学習] --> B["バギング<br/>（Bagging）"]
                            A --> C["ブースティング<br/>（Boosting）"]
                            A --> D["スタッキング<br/>（Stacking）"]
                            B --> E["並列に独立して<br/>学習"]
                            C --> F["逐次的に<br/>弱点を補強"]
                            D --> G["階層的に<br/>組み合わせ"]
                            style A fill:#00897B,color:#fff
                    </div>

                    <h3 class="section-title">8.2 バギング（Bagging）</h3>
                    <p>
                        <strong>バギング（Bootstrap Aggregating）</strong>は、
                        訓練データからランダムにサンプリング（ブートストラップサンプリング）して
                        複数のモデルを並列に訓練し、その予測を平均または多数決で統合する手法です。
                    </p>

                    <h4>バギングの仕組み</h4>
                    <ol>
                        <li>訓練データから復元抽出でN個のサブセットを作成</li>
                        <li>各サブセットで独立にモデルを訓練</li>
                        <li>回帰問題では予測の平均、分類問題では多数決で最終予測</li>
                    </ol>

                    <div class="mermaid">
                        flowchart LR
                            A["訓練データ"] --> B["サブセット1"]
                            A --> C["サブセット2"]
                            A --> D["サブセット3"]
                            B --> E["モデル1"]
                            C --> F["モデル2"]
                            D --> G["モデル3"]
                            E --> H["平均/多数決"]
                            F --> H
                            G --> H
                            H --> I["最終予測"]
                            style A fill:#e3f2fd
                            style H fill:#c8e6c9
                            style I fill:#fff9c4
                    </div>

                    <h4>ランダムフォレスト</h4>
                    <p>
                        <strong>ランダムフォレスト（Random Forest）</strong>は、
                        バギングを決定木に適用し、さらに各分岐で使う特徴量もランダムに選ぶことで
                        多様性を高めた強力なアルゴリズムです。
                    </p>

                    <h5>ランダムフォレストの特徴</h5>
                    <ul>
                        <li><strong>高精度</strong>: 多くの問題で優れた性能</li>
                        <li><strong>過学習に強い</strong>: 多数の木を平均するため安定</li>
                        <li><strong>特徴量重要度</strong>: どの特徴が重要かを自動計算</li>
                        <li><strong>並列処理可能</strong>: 各木が独立なので高速化しやすい</li>
                    </ul>

                    <div class="exercise-container">
                        <h5>実習 8-1: ランダムフォレストの実装</h5>
                        <p>ランダムフォレストで分類モデルを構築します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# データ分割
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# ランダムフォレストモデル
rf = RandomForestClassifier(
    n_estimators=100,      # 木の数
    max_depth=10,          # 各木の最大深さ
    min_samples_split=5,   # 分割に必要な最小サンプル数
    random_state=42
)

# 訓練
rf.fit(X_train, y_train)

# 評価
train_score = rf.score(X_train, y_train)
test_score = rf.score(X_test, y_test)

print(f"訓練精度: {train_score:.3f}")
print(f"テスト精度: {test_score:.3f}")</code></pre>
                        <h6>重要なパラメータ</h6>
                        <ul>
                            <li><strong>n_estimators</strong>: 木の数。多いほど精度が上がるが計算時間も増加</li>
                            <li><strong>max_depth</strong>: 各木の深さ。深すぎると過学習</li>
                            <li><strong>max_features</strong>: 各分岐で考慮する特徴量数</li>
                        </ul>
                    </div>

                    <h3 class="section-title">8.3 ブースティング（Boosting）</h3>
                    <p>
                        <strong>ブースティング</strong>は、弱いモデルを逐次的に訓練し、
                        前のモデルの誤りを次のモデルが修正していく手法です。
                        バギングと異なり、モデルは独立ではなく、前のモデルに依存します。
                    </p>

                    <h4>ブースティングの仕組み</h4>
                    <ol>
                        <li>最初のモデルを訓練</li>
                        <li>誤分類したサンプルに注目</li>
                        <li>誤分類サンプルの重みを増やして次のモデルを訓練</li>
                        <li>すべてのモデルを重み付き多数決で統合</li>
                    </ol>

                    <div class="mermaid">
                        flowchart TD
                            A["モデル1<br/>訓練"] --> B["誤分類を<br/>特定"]
                            B --> C["重みを調整"]
                            C --> D["モデル2<br/>訓練"]
                            D --> E["誤分類を<br/>特定"]
                            E --> F["重みを調整"]
                            F --> G["モデル3<br/>訓練"]
                            G --> H["重み付き<br/>多数決"]
                            style A fill:#c8e6c9
                            style D fill:#c8e6c9
                            style G fill:#c8e6c9
                            style H fill:#fff9c4
                    </div>

                    <h4>AdaBoost</h4>
                    <p>
                        <strong>AdaBoost（Adaptive Boosting）</strong>は、
                        最も古典的なブースティングアルゴリズムです。
                        誤分類されたサンプルの重みを徐々に増やし、難しいサンプルに集中学習します。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 8-2: AdaBoostの実装</h5>
                        <p>AdaBoostで分類モデルを構築します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# ベースモデル（弱学習器）として浅い決定木を使用
base_model = DecisionTreeClassifier(max_depth=1)

# AdaBoostモデル
ada = AdaBoostClassifier(
    base_estimator=base_model,
    n_estimators=50,  # 弱学習器の数
    learning_rate=1.0,
    random_state=42
)

# 訓練と評価
ada.fit(X_train, y_train)
print(f"テスト精度: {ada.score(X_test, y_test):.3f}")</code></pre>
                        <h6>特徴</h6>
                        <p>
                            AdaBoostは弱い学習器を組み合わせて強い学習器を作ります。
                            learning_rateで各モデルの寄与度を調整できます。
                        </p>
                    </div>

                    <h4>Gradient Boosting</h4>
                    <p>
                        <strong>勾配ブースティング（Gradient Boosting）</strong>は、
                        残差（予測誤差）を直接モデル化する高度なブースティング手法です。
                    </p>

                    <h5>勾配ブースティングの考え方</h5>
                    <ol>
                        <li>最初のモデルで予測</li>
                        <li>実際の値と予測値の差（残差）を計算</li>
                        <li>次のモデルはこの残差を予測するように訓練</li>
                        <li>すべてのモデルの予測を足し合わせて最終予測</li>
                    </ol>

                    <div class="exercise-container">
                        <h5>実習 8-3: Gradient Boostingの実装</h5>
                        <p>勾配ブースティングで高精度モデルを構築します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boostingモデル
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,  # 学習率（小さいほど慎重に学習）
    max_depth=3,
    random_state=42
)

# 訓練と評価
gb.fit(X_train, y_train)
print(f"テスト精度: {gb.score(X_test, y_test):.3f}")</code></pre>
                        <h6>パラメータ調整のコツ</h6>
                        <ul>
                            <li>learning_rateを小さくし、n_estimatorsを大きくすると精度向上</li>
                            <li>max_depthは3～5が一般的</li>
                            <li>過学習の兆候があればmax_depthを減らす</li>
                        </ul>
                    </div>

                    <h3 class="section-title">8.4 XGBoost</h3>
                    <p>
                        <strong>XGBoost（eXtreme Gradient Boosting）</strong>は、
                        勾配ブースティングを高速化・高精度化した最も人気のあるアルゴリズムの1つです。
                        Kaggleコンペティションで圧倒的な実績を持ちます。
                    </p>

                    <h4>XGBoostの特徴</h4>
                    <ul>
                        <li><strong>高速</strong>: 並列処理とアルゴリズムの最適化で高速動作</li>
                        <li><strong>高精度</strong>: 正則化により汎化性能が高い</li>
                        <li><strong>欠損値対応</strong>: 欠損値を自動で処理</li>
                        <li><strong>Early Stopping</strong>: 過学習を検出して自動停止</li>
                        <li><strong>柔軟性</strong>: 豊富なパラメータでチューニング可能</li>
                    </ul>

                    <div class="exercise-container">
                        <h5>実習 8-4: XGBoostの実装</h5>
                        <p>XGBoostで高性能モデルを構築します。</p>
                        <h6>インストール</h6>
                        <pre class="code-block"><code class="language-bash">pip install xgboost</code></pre>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">import xgboost as xgb
from sklearn.metrics import accuracy_score

# DMatrix形式に変換（XGBoost用のデータ構造）
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# パラメータ設定
params = {
    'max_depth': 3,
    'eta': 0.1,           # 学習率
    'objective': 'binary:logistic',  # 二値分類
    'eval_metric': 'logloss'
}

# Early Stoppingを使った訓練
evals = [(dtrain, 'train'), (dtest, 'test')]
model = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    evals=evals,
    early_stopping_rounds=10,  # 10回改善なしで停止
    verbose_eval=False
)

# 予測
y_pred_proba = model.predict(dtest)
y_pred = (y_pred_proba > 0.5).astype(int)
print(f"テスト精度: {accuracy_score(y_test, y_pred):.3f}")</code></pre>
                        <h6>重要なパラメータ</h6>
                        <ul>
                            <li><strong>max_depth</strong>: 木の深さ（3～10が一般的）</li>
                            <li><strong>eta</strong>: 学習率（0.01～0.3）</li>
                            <li><strong>subsample</strong>: データのサンプリング率（0.8推奨）</li>
                            <li><strong>colsample_bytree</strong>: 特徴量のサンプリング率</li>
                        </ul>
                    </div>

                    <h3 class="section-title">8.5 LightGBM</h3>
                    <p>
                        <strong>LightGBM（Light Gradient Boosting Machine）</strong>は、
                        Microsoftが開発した超高速な勾配ブースティングライブラリです。
                        XGBoostより高速で、大規模データに特に強いです。
                    </p>

                    <h4>LightGBMの特徴</h4>
                    <ul>
                        <li><strong>超高速</strong>: Leaf-wiseの木成長戦略で高速化</li>
                        <li><strong>メモリ効率</strong>: 大規模データを効率的に処理</li>
                        <li><strong>カテゴリ変数対応</strong>: One-Hotエンコーディング不要</li>
                        <li><strong>高精度</strong>: XGBoostと同等以上の精度</li>
                    </ul>

                    <div class="mermaid">
                        flowchart TD
                            A[勾配ブースティング系] --> B[Gradient Boosting]
                            A --> C[XGBoost]
                            A --> D[LightGBM]
                            B --> E["基本実装<br/>やや遅い"]
                            C --> F["高精度<br/>中速"]
                            D --> G["高精度<br/>超高速"]
                            style A fill:#00897B,color:#fff
                    </div>

                    <div class="exercise-container">
                        <h5>実習 8-5: LightGBMの実装</h5>
                        <p>LightGBMで高速モデルを構築します。</p>
                        <h6>インストール</h6>
                        <pre class="code-block"><code class="language-bash">pip install lightgbm</code></pre>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">import lightgbm as lgb

# Dataset形式に変換
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# パラメータ設定
params = {
    'objective': 'binary',
    'metric': 'binary_logloss',
    'num_leaves': 31,    # 葉の最大数
    'learning_rate': 0.1,
    'feature_fraction': 0.9
}

# 訓練（Early Stopping付き）
model = lgb.train(
    params,
    train_data,
    num_boost_round=100,
    valid_sets=[test_data],
    early_stopping_rounds=10,
    verbose_eval=False
)

# 予測
y_pred = model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)
print(f"テスト精度: {accuracy_score(y_test, y_pred_binary):.3f}")</code></pre>
                        <h6>LightGBM vs XGBoost</h6>
                        <ul>
                            <li><strong>速度</strong>: LightGBMの方が高速（特に大規模データ）</li>
                            <li><strong>精度</strong>: ほぼ同等、問題によって異なる</li>
                            <li><strong>使い分け</strong>: まずLightGBMを試し、必要に応じてXGBoost</li>
                        </ul>
                    </div>

                    <h3 class="section-title">8.6 スタッキング（Stacking）</h3>
                    <p>
                        <strong>スタッキング</strong>は、複数の異なるモデルの予測を
                        別のモデル（メタモデル）で統合する高度なアンサンブル手法です。
                    </p>

                    <h4>スタッキングの仕組み</h4>
                    <ol>
                        <li>第1層：複数の異なるモデル（ベースモデル）を訓練</li>
                        <li>各ベースモデルの予測を新しい特徴量として使用</li>
                        <li>第2層：メタモデルがベースモデルの予測を統合</li>
                    </ol>

                    <div class="mermaid">
                        flowchart TD
                            A["訓練データ"] --> B["モデル1<br/>（決定木）"]
                            A --> C["モデル2<br/>（SVM）"]
                            A --> D["モデル3<br/>（k-NN）"]
                            B --> E["予測1"]
                            C --> F["予測2"]
                            D --> G["予測3"]
                            E --> H["メタモデル<br/>（ロジスティック回帰）"]
                            F --> H
                            G --> H
                            H --> I["最終予測"]
                            style H fill:#00897B,color:#fff
                            style I fill:#fff9c4
                    </div>

                    <div class="exercise-container">
                        <h5>実習 8-6: スタッキングの実装</h5>
                        <p>複数モデルをスタッキングで統合します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# ベースモデルの定義
base_models = [
    ('dt', DecisionTreeClassifier(max_depth=5)),
    ('svm', SVC(probability=True)),
    ('knn', KNeighborsClassifier(n_neighbors=5))
]

# メタモデルの定義
meta_model = LogisticRegression()

# スタッキングモデル
stacking = StackingClassifier(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5  # 交差検証でベースモデルの予測を作成
)

# 訓練と評価
stacking.fit(X_train, y_train)
score = stacking.score(X_test, y_test)
print(f"スタッキングモデル精度: {score:.3f}")</code></pre>
                        <h6>スタッキングのコツ</h6>
                        <ul>
                            <li>多様なモデルを組み合わせる（決定木、線形、距離ベースなど）</li>
                            <li>メタモデルはシンプルなものが良い（ロジスティック回帰など）</li>
                            <li>交差検証で過学習を防ぐ</li>
                        </ul>
                    </div>

                    <div class="warning">
                        <h5>注意：アンサンブルの落とし穴</h5>
                        <ul>
                            <li><strong>計算コスト</strong>: 複数モデルの訓練・予測で時間がかかる</li>
                            <li><strong>解釈性の低下</strong>: モデルが複雑になり説明が困難</li>
                            <li><strong>過学習リスク</strong>: スタッキングなどは適切な検証が必要</li>
                            <li><strong>メンテナンス</strong>: 複数モデルの管理が大変</li>
                        </ul>
                    </div>

                    <div class="quiz-container">
                        <h5>理解度確認クイズ</h5>
                        <ol>
                            <li>
                                <strong>アンサンブル学習の利点</strong><br>
                                なぜ複数のモデルを組み合わせると精度が向上するのですか？
                            </li>
                            <li>
                                <strong>バギングとブースティングの違い</strong><br>
                                ランダムフォレストとGradient Boostingの学習プロセスの違いを説明してください。
                            </li>
                            <li>
                                <strong>ランダムフォレストのパラメータ</strong><br>
                                n_estimatorsを増やすとどのような効果がありますか？
                            </li>
                            <li>
                                <strong>XGBoostとLightGBM</strong><br>
                                両者の特徴と使い分けについて説明してください。
                            </li>
                            <li>
                                <strong>スタッキング</strong><br>
                                スタッキングでベースモデルに多様性が重要な理由は何ですか？
                            </li>
                            <li>
                                <strong>Early Stopping</strong><br>
                                Early Stoppingがなぜ過学習防止に有効なのですか？
                            </li>
                        </ol>
                    </div>

                    <h3 class="section-title">8.7 まとめ</h3>
                    <div class="highlight">
                        <h5>本章で学んだこと</h5>
                        <ul>
                            <li>アンサンブル学習は複数モデルを組み合わせて高精度を実現</li>
                            <li>バギング（ランダムフォレスト）は並列に独立モデルを訓練</li>
                            <li>ブースティングは逐次的に弱点を補強するモデル</li>
                            <li>XGBoostとLightGBMは実務で最も使われる高性能アルゴリズム</li>
                            <li>スタッキングは異なるモデルを階層的に統合</li>
                            <li>適切なパラメータチューニングとEarly Stoppingが重要</li>
                            <li>アンサンブルは精度と解釈性のトレードオフを考慮</li>
                        </ul>
                    </div>

                    <div class="d-flex justify-content-between mt-4">
                        <a href="machine-learning-learning-material-7.html" class="btn btn-secondary">← 前の章</a>
                        <a href="machine-learning-learning-material-9.html" class="btn btn-primary">次の章 →</a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>
本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
