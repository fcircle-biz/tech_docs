<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>機械学習学習教材 第7章 - 特徴量エンジニアリング</title>

    <!-- Bootstrap 5 CDN -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Google Fonts - Noto Sans JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Highlight.js CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <!-- Mermaid.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        body {
            font-family: 'Noto Sans JP', sans-serif;
            padding-top: 56px;
        }

        .navbar {
            background-color: #00897B;
        }

        .sidebar {
            position: sticky;
            top: 70px;
            height: calc(100vh - 70px);
            overflow-y: auto;
            background-color: #f8f9fa;
            padding: 1rem;
        }

        .chapter-title {
            color: #00897B;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            border-bottom: 3px solid #00897B;
            padding-bottom: 0.5rem;
        }

        .section-title {
            color: #26a69a;
            margin-top: 1.5rem;
            margin-bottom: 1rem;
            padding-left: 0.5rem;
            border-left: 4px solid #26a69a;
        }

        .quiz-container {
            background-color: #e0f2f1;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #00897B;
        }

        .exercise-container {
            background-color: #f3e5f5;
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-left: 4px solid #9c27b0;
        }

        .highlight {
            background-color: #fff9c4;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #fbc02d;
        }

        .warning {
            background-color: #ffebee;
            border-radius: 8px;
            padding: 1rem;
            margin: 1rem 0;
            border-left: 4px solid #f44336;
        }

        .code-block {
            background-color: #1e1e1e;
            color: white;
            border-radius: 5px;
            padding: 1rem;
            margin: 1rem 0;
        }

        .nav-link.active {
            background-color: #00897B !important;
            color: white !important;
            border-radius: 5px;
        }

        .nav-link {
            color: #333;
            transition: all 0.3s;
        }

        .nav-link:hover {
            background-color: #e0f2f1;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="README.html">
                <strong>機械学習学習教材</strong>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
    </nav>

    <div class="container-fluid">
        <div class="row">
            <!-- サイドバー -->
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted">
                        <span>学習章</span>
                    </h6>
                    <ul class="nav flex-column">
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-1.html">
                                第1章: 機械学習の概要と環境構築
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-2.html">
                                第2章: 機械学習の基礎概念
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-3.html">
                                第3章: データの前処理
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-4.html">
                                第4章: 教師あり学習：回帰
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-5.html">
                                第5章: 教師あり学習：分類
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-6.html">
                                第6章: モデルの評価と検証
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="machine-learning-learning-material-7.html">
                                第7章: 特徴量エンジニアリング
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-8.html">
                                第8章: アンサンブル学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-9.html">
                                第9章: 教師なし学習
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="machine-learning-learning-material-10.html">
                                第10章: 実践プロジェクト開発
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>

            <!-- メインコンテンツ -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2">第7章: 特徴量エンジニアリング</h1>
                </div>

                <div id="chapter7">
                    <h2 class="chapter-title">データから価値ある特徴を引き出す技術</h2>

                    <div class="highlight">
                        <h5>この章で学ぶこと</h5>
                        <ul>
                            <li>特徴量エンジニアリングの重要性を理解する</li>
                            <li>特徴量の重要度分析で有効な特徴を特定する</li>
                            <li>特徴量選択で不要な特徴を排除する</li>
                            <li>新しい特徴量を生成して表現力を高める</li>
                            <li>次元削減で高次元データを扱いやすくする</li>
                            <li>時系列特徴量を作成する</li>
                        </ul>
                    </div>

                    <h3 class="section-title">7.1 特徴量エンジニアリングとは</h3>
                    <p>
                        <strong>特徴量エンジニアリング（Feature Engineering）</strong>は、
                        生データから機械学習モデルにとって有用な特徴量を作り出すプロセスです。
                        「データとアルゴリズムが同じなら、特徴量が勝敗を分ける」と言われるほど重要です。
                    </p>

                    <h4>なぜ特徴量エンジニアリングが重要か</h4>
                    <ul>
                        <li><strong>モデル性能の向上</strong>: 良い特徴量は単純なモデルでも高い精度を実現</li>
                        <li><strong>学習の効率化</strong>: 適切な特徴量で学習が速く収束</li>
                        <li><strong>解釈性の向上</strong>: 意味のある特徴量でモデルが理解しやすくなる</li>
                        <li><strong>過学習の抑制</strong>: 不要な特徴を除去することで汎化性能が向上</li>
                    </ul>

                    <div class="mermaid">
                        flowchart LR
                            A["生データ"] --> B["特徴量<br/>エンジニアリング"]
                            B --> C["有用な特徴量"]
                            C --> D["機械学習<br/>モデル"]
                            D --> E["高精度な予測"]
                            B --> F["特徴量分析"]
                            B --> G["特徴量選択"]
                            B --> H["特徴量生成"]
                            style A fill:#e3f2fd
                            style B fill:#c8e6c9
                            style C fill:#fff9c4
                            style E fill:#c8e6c9
                    </div>

                    <h3 class="section-title">7.2 特徴量の重要度分析</h3>
                    <p>
                        どの特徴量がモデルの予測に大きく貢献しているかを定量化することで、
                        データへの理解を深め、重要な特徴に焦点を当てることができます。
                    </p>

                    <h4>木ベースモデルの特徴量重要度</h4>
                    <p>
                        決定木やランダムフォレストは、各特徴量がどれだけ情報利得に貢献したかを
                        自動的に計算します。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-1: 特徴量重要度の可視化</h5>
                        <p>ランダムフォレストを使って特徴量の重要度を分析します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import pandas as pd

# ランダムフォレストモデルの訓練
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 特徴量重要度の取得
importances = rf.feature_importances_
feature_names = ['特徴1', '特徴2', '特徴3', '特徴4']

# DataFrameで整理
importance_df = pd.DataFrame({
    '特徴量': feature_names,
    '重要度': importances
}).sort_values('重要度', ascending=False)

print(importance_df)

# 可視化
plt.barh(importance_df['特徴量'], importance_df['重要度'])
plt.xlabel('重要度')
plt.title('特徴量重要度')
plt.show()</code></pre>
                        <h6>解釈</h6>
                        <p>
                            重要度が高い特徴量は、モデルの予測に大きく寄与しています。
                            重要度が極端に低い特徴量は、削除を検討できます。
                        </p>
                    </div>

                    <h4>線形モデルの係数</h4>
                    <p>
                        ロジスティック回帰などの線形モデルでは、
                        係数の絶対値が特徴量の重要性を示します。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-2: 線形モデルの係数分析</h5>
                        <p>ロジスティック回帰の係数から特徴量の影響を確認します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.linear_model import LogisticRegression

# 標準化したデータでモデル訓練（係数の比較のため）
lr = LogisticRegression()
lr.fit(X_train_scaled, y_train)

# 係数の取得と可視化
coef_df = pd.DataFrame({
    '特徴量': feature_names,
    '係数': lr.coef_[0]
}).sort_values('係数', key=abs, ascending=False)

print(coef_df)

# 係数の可視化
plt.barh(coef_df['特徴量'], coef_df['係数'])
plt.xlabel('係数')
plt.title('線形モデルの係数')
plt.axvline(x=0, color='black', linestyle='--')
plt.show()</code></pre>
                        <h6>注意点</h6>
                        <p>
                            係数を比較する場合は、特徴量のスケールを揃える必要があります。
                            正の係数は陽性予測を、負の係数は陰性予測を強める方向に働きます。
                        </p>
                    </div>

                    <h3 class="section-title">7.3 特徴量選択</h3>
                    <p>
                        <strong>特徴量選択（Feature Selection）</strong>は、
                        多数の特徴量から重要なものだけを選び出すプロセスです。
                        不要な特徴量を削除することで、過学習を防ぎ、計算コストを削減できます。
                    </p>

                    <h4>特徴量選択の主な手法</h4>

                    <div class="mermaid">
                        flowchart TD
                            A[特徴量選択手法] --> B["フィルター法<br/>（統計指標で選択）"]
                            A --> C["ラッパー法<br/>（モデル性能で選択）"]
                            A --> D["組み込み法<br/>（モデル訓練中に選択）"]
                            B --> E["相関係数<br/>分散分析など"]
                            C --> F["再帰的特徴除去<br/>RFE"]
                            D --> G["Lasso回帰<br/>木の重要度"]
                            style A fill:#00897B,color:#fff
                    </div>

                    <h4>1. フィルター法：分散による選択</h4>
                    <p>
                        分散が小さい（ほとんど同じ値）特徴量は情報量が少ないため削除します。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-3: 低分散特徴量の除去</h5>
                        <p>分散が閾値以下の特徴量を削除します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.feature_selection import VarianceThreshold

# 分散が0.1以下の特徴量を削除
selector = VarianceThreshold(threshold=0.1)
X_high_variance = selector.fit_transform(X)

print(f"元の特徴量数: {X.shape[1]}")
print(f"選択後の特徴量数: {X_high_variance.shape[1]}")
print(f"削除された特徴量: {selector.get_support()}")</code></pre>
                    </div>

                    <h4>2. ラッパー法：再帰的特徴除去（RFE）</h4>
                    <p>
                        モデルを繰り返し訓練しながら、最も重要度の低い特徴量を順次削除していきます。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-4: RFEによる特徴量選択</h5>
                        <p>重要な特徴量を自動的に選択します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# 5つの特徴量を選択
model = LogisticRegression()
rfe = RFE(estimator=model, n_features_to_select=5)
X_selected = rfe.fit_transform(X_train, y_train)

# 選択された特徴量
selected_features = [name for name, selected in
                    zip(feature_names, rfe.support_) if selected]
print(f"選択された特徴量: {selected_features}")
print(f"特徴量ランキング: {rfe.ranking_}")</code></pre>
                        <h6>解説</h6>
                        <p>
                            ranking_が1の特徴量が最も重要です。
                            RFEは計算コストが高いですが、モデルの性能を考慮した選択ができます。
                        </p>
                    </div>

                    <h4>3. 組み込み法：Lassoによる選択</h4>
                    <p>
                        Lasso回帰は、不要な特徴量の係数を自動的に0にするため、
                        特徴量選択の効果があります。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-5: Lassoによる特徴量選択</h5>
                        <p>Lassoで自動的に特徴量を選択します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.linear_model import LassoCV

# Lassoで最適なαを自動選択
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_train, y_train)

# 係数が0でない特徴量を取得
selected_mask = lasso.coef_ != 0
selected_features = [name for name, mask in
                    zip(feature_names, selected_mask) if mask]

print(f"選択された特徴量数: {sum(selected_mask)}")
print(f"選択された特徴量: {selected_features}")</code></pre>
                    </div>

                    <h3 class="section-title">7.4 新しい特徴量の生成</h3>
                    <p>
                        既存の特徴量を組み合わせたり変換したりして、
                        新しい有用な特徴量を作り出すことができます。
                    </p>

                    <h4>特徴量生成の主な手法</h4>

                    <h5>1. 四則演算による組み合わせ</h5>
                    <ul>
                        <li><strong>比率</strong>: 売上 / 広告費 = ROI</li>
                        <li><strong>差分</strong>: 最高気温 - 最低気温 = 日較差</li>
                        <li><strong>積</strong>: 面積 × 駅距離 = 立地指数</li>
                        <li><strong>和</strong>: 国語 + 数学 + 英語 = 合計点</li>
                    </ul>

                    <h5>2. 多項式特徴量</h5>
                    <p>
                        特徴量の2乗、3乗、交互作用項を追加して表現力を高めます。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-6: 多項式特徴量の生成</h5>
                        <p>既存の特徴量から多項式特徴量を作成します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.preprocessing import PolynomialFeatures

# 2次の多項式特徴量を生成
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# 生成された特徴量名
feature_names_poly = poly.get_feature_names_out()
print(f"元の特徴量数: {X.shape[1]}")
print(f"多項式特徴量数: {X_poly.shape[1]}")
print(f"生成された特徴量: {feature_names_poly[:10]}")</code></pre>
                        <h6>注意</h6>
                        <p>
                            次数を上げすぎると特徴量が爆発的に増加し、過学習のリスクが高まります。
                            degree=2または3が実用的です。
                        </p>
                    </div>

                    <h5>3. ビニング（離散化）</h5>
                    <p>
                        連続値を区間に分けてカテゴリ化します。
                        非線形パターンを捉えやすくなります。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-7: 連続値のビニング</h5>
                        <p>年齢を年代別カテゴリに変換します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">import pandas as pd

# サンプルデータ
ages = [22, 35, 47, 51, 28, 63, 19, 45]

# 年代別にビニング
age_bins = [0, 30, 40, 50, 100]
age_labels = ['20代以下', '30代', '40代', '50代以上']
age_categories = pd.cut(ages, bins=age_bins, labels=age_labels)

print(age_categories)</code></pre>
                        <h6>使いどころ</h6>
                        <p>
                            年齢、収入、距離などの連続値で、区間ごとに異なるパターンがある場合に有効です。
                        </p>
                    </div>

                    <h5>4. カテゴリ変数の変換</h5>
                    <p>
                        カテゴリ変数を数値に変換する手法です。
                    </p>
                    <ul>
                        <li><strong>One-Hot Encoding</strong>: カテゴリごとにバイナリ列を作成</li>
                        <li><strong>Label Encoding</strong>: カテゴリを整数にマッピング</li>
                        <li><strong>Target Encoding</strong>: カテゴリごとの目的変数の平均を使用</li>
                    </ul>

                    <div class="warning">
                        <h5>注意：データ漏洩に注意</h5>
                        <p>
                            Target Encodingなど、目的変数を使った特徴量生成は、
                            訓練データで計算した値をテストデータに適用する必要があります。
                            テストデータで直接計算するとデータ漏洩が発生します。
                        </p>
                    </div>

                    <h3 class="section-title">7.5 次元削減</h3>
                    <p>
                        <strong>次元削減（Dimensionality Reduction）</strong>は、
                        高次元データを低次元に変換しながら、重要な情報を保持する技術です。
                    </p>

                    <h4>次元削減の利点</h4>
                    <ul>
                        <li><strong>可視化</strong>: 2次元または3次元に削減してデータを可視化</li>
                        <li><strong>計算効率</strong>: 特徴量が減ることで訓練が高速化</li>
                        <li><strong>過学習防止</strong>: 冗長な情報を削減</li>
                        <li><strong>ノイズ除去</strong>: 主要なパターンのみを抽出</li>
                    </ul>

                    <h4>PCA（主成分分析）</h4>
                    <p>
                        <strong>PCA（Principal Component Analysis）</strong>は、
                        データの分散が最大となる方向（主成分）を見つけ、
                        その方向に射影することで次元を削減します。
                    </p>

                    <div class="mermaid">
                        flowchart LR
                            A["高次元データ<br/>（例: 100次元）"] --> B["PCA変換"]
                            B --> C["低次元データ<br/>（例: 10次元）"]
                            B --> D["分散の大きい<br/>主成分を選択"]
                            D --> C
                            style A fill:#e3f2fd
                            style B fill:#c8e6c9
                            style C fill:#fff9c4
                    </div>

                    <div class="exercise-container">
                        <h5>実習 7-8: PCAによる次元削減</h5>
                        <p>高次元データを2次元に削減して可視化します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 2次元に削減
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 説明された分散の割合
print(f"第1主成分の分散寄与率: {pca.explained_variance_ratio_[0]:.3f}")
print(f"第2主成分の分散寄与率: {pca.explained_variance_ratio_[1]:.3f}")
print(f"累積寄与率: {sum(pca.explained_variance_ratio_):.3f}")

# 可視化
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.xlabel('第1主成分')
plt.ylabel('第2主成分')
plt.title('PCAによる2次元可視化')
plt.colorbar(label='クラス')
plt.show()</code></pre>
                        <h6>主成分数の選び方</h6>
                        <p>
                            累積寄与率が80-95%になる主成分数を選ぶのが一般的です。
                            スクリープロットで寄与率の変化を確認するのも有効です。
                        </p>
                    </div>

                    <h4>t-SNE（t分布型確率的近傍埋め込み）</h4>
                    <p>
                        <strong>t-SNE</strong>は、高次元データの局所的な構造を保ちながら
                        2次元または3次元に可視化する手法です。PCAより複雑なパターンを捉えられます。
                    </p>

                    <div class="exercise-container">
                        <h5>実習 7-9: t-SNEによる可視化</h5>
                        <p>t-SNEでデータの構造を可視化します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">from sklearn.manifold import TSNE

# t-SNEで2次元に削減
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_scaled)

# 可視化
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.xlabel('t-SNE 次元1')
plt.ylabel('t-SNE 次元2')
plt.title('t-SNEによる可視化')
plt.colorbar(label='クラス')
plt.show()</code></pre>
                        <h6>注意点</h6>
                        <ul>
                            <li>t-SNEは計算コストが高く、大規模データには時間がかかる</li>
                            <li>可視化専用で、変換後のデータを予測に使うのは推奨されない</li>
                            <li>perplexityパラメータで結果が変わるため、複数試す</li>
                        </ul>
                    </div>

                    <h3 class="section-title">7.6 時系列特徴量</h3>
                    <p>
                        時系列データでは、時間に関する特徴量を作成することで
                        予測性能を大きく向上できます。
                    </p>

                    <h4>時系列特徴量の種類</h4>
                    <ul>
                        <li><strong>時間軸の分解</strong>: 年、月、日、曜日、時間、四半期など</li>
                        <li><strong>周期性</strong>: 週末フラグ、祝日フラグ、季節など</li>
                        <li><strong>ラグ特徴量</strong>: 過去の値（1日前、7日前、30日前など）</li>
                        <li><strong>移動平均</strong>: 過去N日間の平均</li>
                        <li><strong>変化率</strong>: 前日比、前年比など</li>
                        <li><strong>累積統計</strong>: 累積和、累積最大値など</li>
                    </ul>

                    <div class="exercise-container">
                        <h5>実習 7-10: 時系列特徴量の生成</h5>
                        <p>日付データから様々な時系列特徴量を作成します。</p>
                        <h6>実装例</h6>
                        <pre class="code-block"><code class="language-python">import pandas as pd

# サンプルデータ
df = pd.DataFrame({
    'date': pd.date_range('2024-01-01', periods=100),
    'sales': np.random.randint(100, 200, 100)
})

# 日付から特徴量を抽出
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['dayofweek'] = df['date'].dt.dayofweek  # 0=月曜日
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

# ラグ特徴量
df['sales_lag1'] = df['sales'].shift(1)  # 1日前
df['sales_lag7'] = df['sales'].shift(7)  # 7日前

# 移動平均
df['sales_ma7'] = df['sales'].rolling(window=7).mean()

# 変化率
df['sales_change'] = df['sales'].pct_change()

print(df.head(10))</code></pre>
                        <h6>活用場面</h6>
                        <p>
                            売上予測、需要予測、株価予測など、時間的なパターンがあるデータで
                            これらの特徴量は非常に有効です。
                        </p>
                    </div>

                    <div class="quiz-container">
                        <h5>理解度確認クイズ</h5>
                        <ol>
                            <li>
                                <strong>特徴量エンジニアリングの重要性</strong><br>
                                なぜ特徴量エンジニアリングがモデル性能に大きく影響するのですか？
                            </li>
                            <li>
                                <strong>特徴量重要度</strong><br>
                                ランダムフォレストの特徴量重要度はどのように計算されますか？
                            </li>
                            <li>
                                <strong>特徴量選択の手法</strong><br>
                                フィルター法、ラッパー法、組み込み法の違いを説明してください。
                            </li>
                            <li>
                                <strong>多項式特徴量</strong><br>
                                多項式特徴量を生成する利点と注意点は何ですか？
                            </li>
                            <li>
                                <strong>PCAとt-SNE</strong><br>
                                PCAとt-SNEの違いは何ですか？それぞれどのような場合に使いますか？
                            </li>
                            <li>
                                <strong>時系列特徴量</strong><br>
                                ラグ特徴量を作成する際の注意点は何ですか？
                            </li>
                        </ol>
                    </div>

                    <h3 class="section-title">7.7 まとめ</h3>
                    <div class="highlight">
                        <h5>本章で学んだこと</h5>
                        <ul>
                            <li>特徴量エンジニアリングはモデル性能に決定的な影響を与える</li>
                            <li>特徴量重要度で有効な特徴を特定できる</li>
                            <li>特徴量選択で不要な特徴を削除し、過学習を防ぐ</li>
                            <li>四則演算や多項式変換で新しい特徴量を生成</li>
                            <li>PCAやt-SNEで次元削減と可視化を行う</li>
                            <li>時系列データでは時間関連の特徴量が重要</li>
                            <li>ドメイン知識を活かした特徴量設計が成功の鍵</li>
                        </ul>
                    </div>

                    <div class="d-flex justify-content-between mt-4">
                        <a href="machine-learning-learning-material-6.html" class="btn btn-secondary">← 前の章</a>
                        <a href="machine-learning-learning-material-8.html" class="btn btn-primary">次の章 →</a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>
本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
