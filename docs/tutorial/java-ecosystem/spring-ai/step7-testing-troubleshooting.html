<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spring AI + Ollama チュートリアル ステップ7 - 動作確認とトラブルシューティング</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
    <style>
        :root { --primary-color: #f57c00; --secondary-color: #ff9800; --accent-color: #4caf50; }
        body { font-family: 'Noto Sans JP', sans-serif; background-color: #f8f9fa; padding-top: 56px; }
        .navbar { background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%); }
        .sidebar { position: fixed; top: 56px; bottom: 0; left: 0; z-index: 100; padding: 48px 0 0; background-color: #f8f9fa; overflow-y: auto; }
        .sidebar .nav-link { font-weight: 500; color: #333; padding: 0.5rem 1rem; border-left: 3px solid transparent; }
        .sidebar .nav-link:hover { color: var(--primary-color); background-color: #fff3e0; border-left-color: var(--primary-color); }
        .sidebar .nav-link.active { color: var(--primary-color); background-color: #fff3e0; border-left-color: var(--primary-color); font-weight: 700; }
        .chapter-title { color: var(--primary-color); border-bottom: 3px solid var(--primary-color); padding-bottom: 10px; margin-bottom: 30px; font-weight: 700; }
        .section-title { color: var(--secondary-color); margin-top: 30px; margin-bottom: 20px; font-weight: 600; border-left: 4px solid var(--secondary-color); padding-left: 15px; }
        .highlight { background: linear-gradient(to right, #fff3e0, #ffe0b2); border-left: 4px solid var(--primary-color); padding: 20px; margin: 20px 0; border-radius: 5px; }
        .highlight h5 { color: var(--primary-color); font-weight: 700; margin-bottom: 15px; }
        .code-block { background-color: #282c34; border-radius: 5px; padding: 15px; margin: 15px 0; overflow-x: auto; }
        .code-block code { color: #abb2bf; font-family: 'Courier New', monospace; font-size: 0.9rem; }
        .exercise-container { background-color: #f0f7ff; border: 2px solid #2196F3; border-radius: 8px; padding: 25px; margin: 25px 0; }
        .exercise-container h5 { color: #1976D2; font-weight: 700; margin-bottom: 15px; }
        .quiz-container { background-color: #f1f8e9; border: 2px solid var(--accent-color); border-radius: 8px; padding: 25px; margin: 25px 0; }
        .quiz-container h5 { color: var(--accent-color); font-weight: 700; margin-bottom: 15px; }
        .alert-custom { border-radius: 8px; padding: 15px 20px; margin: 15px 0; }
        footer { margin-top: 50px; padding: 30px 0; background-color: #212529; }
        .btn-primary { background-color: var(--primary-color); border-color: var(--primary-color); }
        .btn-primary:hover { background-color: #e65100; border-color: #e65100; }
        .mermaid { background-color: white; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .concept-box { background-color: #e8f5e9; border: 2px solid #4caf50; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .concept-box h6 { color: #2e7d32; font-weight: 700; margin-bottom: 10px; }
        .error-box { background-color: #ffebee; border: 2px solid #f44336; border-radius: 8px; padding: 20px; margin: 20px 0; }
        .error-box h6 { color: #c62828; font-weight: 700; margin-bottom: 10px; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="#"><i class="fab fa-java"></i> <strong>Spring AI + Ollama + Qwen3 実践チュートリアル</strong></a>
        </div>
    </nav>
    <div class="container-fluid">
        <div class="row">
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted"><span>チュートリアルステップ</span></h6>
                    <ul class="nav flex-column">
                        <li class="nav-item"><a class="nav-link" href="step1-environment-setup.html"><i class="fas fa-cog"></i> ステップ1: 環境構築</a></li>
                        <li class="nav-item"><a class="nav-link" href="step2-spring-ai-ollama-integration.html"><i class="fas fa-plug"></i> ステップ2: Spring AI統合</a></li>
                        <li class="nav-item"><a class="nav-link" href="step3-chat-implementation.html"><i class="fas fa-comments"></i> ステップ3: チャット機能実装</a></li>
                        <li class="nav-item"><a class="nav-link" href="step4-thymeleaf-view.html"><i class="fas fa-file-code"></i> ステップ4: Thymeleafビュー</a></li>
                        <li class="nav-item"><a class="nav-link" href="step5-docker-containerization.html"><i class="fab fa-docker"></i> ステップ5: Dockerコンテナ化</a></li>
                        <li class="nav-item"><a class="nav-link" href="step6-docker-compose-integration.html"><i class="fas fa-layer-group"></i> ステップ6: docker-compose統合</a></li>
                        <li class="nav-item"><a class="nav-link active" href="step7-testing-troubleshooting.html"><i class="fas fa-check-circle"></i> ステップ7: 動作確認</a></li>
                    </ul>
                </div>
            </nav>
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2"><i class="fas fa-check-circle"></i> ステップ7: 動作確認とトラブルシューティング</h1>
                </div>
                <div id="step7">
                    <h2 class="chapter-title">システム全体の動作確認とデバッグ手法</h2>
                    <div class="highlight">
                        <h5><i class="fas fa-bullseye"></i> このステップで確認すること</h5>
                        <ul>
                            <li><strong>全体システムの起動確認</strong> - docker-compose upでの一括起動</li>
                            <li><strong>AIチャット機能のテスト</strong> - 実際の質問応答テスト</li>
                            <li><strong>ログ確認とデバッグ</strong> - 各種ログの確認方法</li>
                            <li><strong>よくあるエラーと対処法</strong> - トラブルシューティング</li>
                            <li><strong>パフォーマンス確認</strong> - 応答速度の測定</li>
                            <li><strong>リソース使用状況</strong> - メモリとCPU使用率の監視</li>
                            <li><strong>環境のクリーンアップ</strong> - 不要なリソースの削除方法</li>
                        </ul>
                        <p><strong>所要時間：</strong> 約2時間</p>
                    </div>
                    <h3 class="section-title">7.1 全体システムの起動と動作確認</h3>
                    <p>docker-composeで全環境を一括起動し、正常に動作することを確認します。</p>
                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> 確認 7-1: システム全体の起動確認</h5>
                        <h6><i class="fas fa-list-ol"></i> 確認手順</h6>
                        <ol>
                            <li><strong>全環境の起動</strong>:
                                <pre class="code-block"><code class="language-bash"># プロジェクトルート（ai-chat/）で実行
docker-compose up -d

# 起動状況を確認
docker-compose ps

# ログをリアルタイム監視
docker-compose logs -f</code></pre>
                            </li>
                            <li><strong>各サービスの状態確認</strong>:
                                <pre class="code-block"><code class="language-bash"># ollamaサービスの健全性確認
curl http://localhost:11434/api/tags

# Qwen3:7bモデルの存在確認
docker exec ollama ollama list

# appサービスの動作確認
curl http://localhost:8080/chat</code></pre>
                            </li>
                            <li><strong>ブラウザでアクセス</strong>:
                                <p><code>http://localhost:8080/chat</code> にアクセスして、チャット画面が表示されることを確認します。</p>
                            </li>
                        </ol>
                        <h6><i class="fas fa-thumbs-up"></i> 正常動作の確認ポイント</h6>
                        <div class="alert alert-success">
                            <ul class="mb-0">
                                <li><code>docker-compose ps</code>で両サービスが "Up" 状態</li>
                                <li>ollamaサービスが "(healthy)" 表示</li>
                                <li><code>ollama list</code>でqwen3:7bが表示される</li>
                                <li>ブラウザでチャット画面が表示される</li>
                                <li>メッセージ送信でAI応答が返ってくる</li>
                            </ul>
                        </div>
                    </div>
                    <h3 class="section-title">7.2 AIチャット機能の実践テスト</h3>
                    <p>様々な質問パターンでAIチャット機能をテストし、応答品質を確認します。</p>
                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> テスト 7-2: AIチャット機能のテストケース</h5>
                        <h6><i class="fas fa-vial"></i> テストケース一覧</h6>
                        <table class="table table-bordered">
                            <thead class="table-light">
                                <tr><th>テストケース</th><th>入力例</th><th>期待される動作</th></tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>基本的な質問応答</td>
                                    <td>「Javaの特徴を3つ教えてください」</td>
                                    <td>Javaの主要な特徴（オブジェクト指向、プラットフォーム独立性、ガベージコレクション等）を説明</td>
                                </tr>
                                <tr>
                                    <td>日本語対話</td>
                                    <td>「こんにちは！今日の天気はどうですか？」</td>
                                    <td>自然な日本語で応答（ただし、Qwen3はリアルタイム情報にアクセスできないことを説明）</td>
                                </tr>
                                <tr>
                                    <td>技術的な質問</td>
                                    <td>「Spring Bootとは何ですか？」</td>
                                    <td>Spring Bootの概要と利点を説明</td>
                                </tr>
                                <tr>
                                    <td>コード生成依頼</td>
                                    <td>「PythonでHello Worldを書いてください」</td>
                                    <td>Pythonのコード例を提示</td>
                                </tr>
                                <tr>
                                    <td>長文応答</td>
                                    <td>「機械学習の歴史を詳しく教えてください」</td>
                                    <td>長文の説明（num-predict設定により2000トークンまで生成可能）</td>
                                </tr>
                                <tr>
                                    <td>バリデーションテスト</td>
                                    <td>空白のまま送信</td>
                                    <td>「メッセージを入力してください」エラーメッセージ表示</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                    <h3 class="section-title">7.3 よくあるエラーとトラブルシューティング</h3>
                    <p>チュートリアルでよく遭遇するエラーとその解決方法をまとめます。</p>
                    <div class="error-box">
                        <h6><i class="fas fa-exclamation-triangle"></i> エラー 1: Connection refused (Ollama接続エラー)</h6>
                        <p><strong>症状:</strong> AI応答取得時に "Connection refused: localhost/127.0.0.1:11434" エラー</p>
                        <p><strong>原因:</strong> Ollamaコンテナが起動していない、またはネットワーク設定が正しくない</p>
                        <p><strong>解決方法:</strong></p>
                        <pre class="code-block mb-0"><code class="language-bash"># Ollamaコンテナの状態確認
docker-compose ps ollama

# 起動していない場合
docker-compose start ollama

# ログ確認
docker-compose logs ollama

# ネットワーク接続テスト
docker exec ai-chat-app curl http://ollama:11434/api/tags</code></pre>
                    </div>
                    <div class="error-box">
                        <h6><i class="fas fa-exclamation-triangle"></i> エラー 2: Model not found (モデル未ダウンロード)</h6>
                        <p><strong>症状:</strong> "Model 'qwen3:7b' not found" エラー</p>
                        <p><strong>原因:</strong> Qwen3:7bモデルがダウンロードされていない</p>
                        <p><strong>解決方法:</strong></p>
                        <pre class="code-block mb-0"><code class="language-bash"># モデル一覧確認
docker exec ollama ollama list

# qwen3:7bが無い場合、手動でダウンロード
docker exec ollama ollama pull qwen3:7b

# ダウンロード進行状況確認
docker exec ollama ollama list</code></pre>
                    </div>
                    <div class="error-box">
                        <h6><i class="fas fa-exclamation-triangle"></i> エラー 3: Out of Memory (メモリ不足)</h6>
                        <p><strong>症状:</strong> Ollamaまたはアプリケーションが突然停止する、応答が非常に遅い</p>
                        <p><strong>原因:</strong> システムメモリが不足している（Qwen3:7bは約4GBのメモリを使用）</p>
                        <p><strong>解決方法:</strong></p>
                        <ul>
                            <li>Docker Desktopの設定でメモリを8GB以上に増やす</li>
                            <li>他のアプリケーションを終了してメモリを解放</li>
                            <li>より小さいモデル（qwen3:3b等）を使用する</li>
                        </ul>
                        <pre class="code-block mb-0"><code class="language-yaml"># docker-compose.ymlでメモリ制限を設定（例）
services:
  ollama:
    mem_limit: 6g
    mem_reservation: 4g</code></pre>
                    </div>
                    <div class="error-box">
                        <h6><i class="fas fa-exclamation-triangle"></i> エラー 4: 応答が遅い（パフォーマンス問題）</h6>
                        <p><strong>症状:</strong> AIの応答に1分以上かかる</p>
                        <p><strong>原因:</strong> CPUでの推論は時間がかかる、システムリソース不足</p>
                        <p><strong>解決方法:</strong></p>
                        <ul class="mb-0">
                            <li><code>num-predict</code>を小さくする（例: 500トークン）</li>
                            <li>より小さいモデルを使用する（qwen3:3b）</li>
                            <li>他のプロセスを終了してCPUリソースを確保</li>
                            <li>GPU対応のOllamaイメージを使用（GPUがある場合）</li>
                        </ul>
                    </div>
                    <h3 class="section-title">7.4 ログ確認とデバッグ手法</h3>
                    <p>問題発生時のログ確認方法とデバッグ手法を学びます。</p>
                    <div class="concept-box">
                        <h6><i class="fas fa-bug"></i> デバッグに役立つコマンド</h6>
                        <pre class="code-block"><code class="language-bash"># 全サービスのログをリアルタイム表示
docker-compose logs -f

# 特定サービスのログのみ表示
docker-compose logs -f app
docker-compose logs -f ollama

# 最新100行のログを表示
docker-compose logs --tail=100 app

# タイムスタンプ付きログ
docker-compose logs -t app

# コンテナ内でシェル起動（デバッグ用）
docker exec -it ai-chat-app sh
docker exec -it ollama bash

# コンテナのリソース使用状況確認
docker stats

# ネットワーク接続確認
docker network inspect ai-chat_ai-chat-network</code></pre>
                    </div>
                    <h3 class="section-title">7.5 パフォーマンスチューニング</h3>
                    <p>AI応答速度を改善するための最適化手法を学びます。</p>
                    <div class="concept-box">
                        <h6><i class="fas fa-rocket"></i> パフォーマンス最適化のポイント</h6>
                        <ol>
                            <li><strong>num-predict を調整</strong>: 長文が不要な場合は500-1000に削減</li>
                            <li><strong>temperature を下げる</strong>: 0.3-0.5にすると応答が速くなる傾向</li>
                            <li><strong>軽量モデルを使用</strong>: qwen3:3b（3Bパラメータ）はより高速</li>
                            <li><strong>システムプロンプトを簡潔に</strong>: 長いシステムプロンプトは処理時間増加</li>
                        </ol>
                        <p><strong>最適化例（application.yml）:</strong></p>
                        <pre class="code-block mb-0"><code class="language-yaml">spring:
  ai:
    ollama:
      chat:
        model: qwen3:3b  # より軽量なモデル
        options:
          temperature: 0.3  # 決定的な応答で高速化
          num-predict: 500  # 短い応答で高速化</code></pre>
                    </div>
                    <h3 class="section-title">7.6 環境のクリーンアップ</h3>
                    <p>開発環境をクリーンアップして、ディスク容量を解放します。</p>
                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> クリーンアップ手順</h5>
                        <pre class="code-block"><code class="language-bash"># 全サービスを停止して削除（ボリュームは保持）
docker-compose down

# ボリュームも含めて完全削除（Ollamaモデルも削除される）
docker-compose down -v

# 未使用のDockerリソースを一括削除
docker system prune -a

# ディスク使用量確認
docker system df</code></pre>
                        <p><strong>注意:</strong> <code>docker-compose down -v</code>を実行すると、Qwen3:7bモデル（約4.7GB）が削除され、次回起動時に再ダウンロードが必要になります。</p>
                    </div>
                    <h3 class="section-title">7.7 チュートリアル完了おめでとうございます！</h3>
                    <div class="quiz-container">
                        <h5><i class="fas fa-trophy"></i> 習得したスキルのまとめ</h5>
                        <p>このチュートリアルを通じて、以下のスキルを習得しました：</p>
                        <ul>
                            <li><strong>Spring AI基礎</strong>: Spring AIフレームワークの基本概念と実装方法</li>
                            <li><strong>Ollama統合</strong>: ローカルLLM環境の構築と管理</li>
                            <li><strong>LLM活用</strong>: Qwen3を使用したAI対話アプリケーション開発</li>
                            <li><strong>Spring Boot開発</strong>: MVC、Thymeleaf、バリデーション</li>
                            <li><strong>Docker技術</strong>: マルチステージビルド、docker-compose</li>
                            <li><strong>プロンプトエンジニアリング</strong>: 効果的なAI応答を引き出す設計</li>
                            <li><strong>トラブルシューティング</strong>: ログ確認とデバッグ手法</li>
                        </ul>
                    </div>
                    <div class="alert alert-info alert-custom">
                        <h6><i class="fas fa-forward"></i> 次のステップ推奨</h6>
                        <p>さらにスキルアップするために、以下の拡張機能に挑戦してみましょう：</p>
                        <ul class="mb-0">
                            <li><strong>ストリーミング対応</strong>: Spring WebFluxでリアルタイムストリーミング応答</li>
                            <li><strong>チャット履歴管理</strong>: データベースに会話履歴を保存</li>
                            <li><strong>RAG実装</strong>: ベクトルデータベースと統合した検索拡張生成</li>
                            <li><strong>マルチモデル対応</strong>: 複数のLLMモデルを切り替え可能に</li>
                            <li><strong>認証機能</strong>: Spring Securityによるユーザー認証</li>
                            <li><strong>API化</strong>: RESTful APIとしてチャット機能を公開</li>
                        </ul>
                    </div>
                    <div class="highlight">
                        <h5><i class="fas fa-graduation-cap"></i> おめでとうございます！</h5>
                        <p>Spring AI + Ollama + Qwen3を使用したAIチャットアプリケーションの開発チュートリアルを完了しました。</p>
                        <p>このチュートリアルで学んだ知識を活用して、さらに高度なAIアプリケーション開発に挑戦してください。</p>
                        <p><strong>Happy AI Coding with Spring! </strong><i class="fas fa-robot"></i><i class="fas fa-rocket"></i></p>
                    </div>
                    <div class="d-flex justify-content-between mt-5 mb-4">
                        <a href="step6-docker-compose-integration.html" class="btn btn-secondary"><i class="fas fa-arrow-left"></i> 前のステップ</a>
                        <a href="README.html" class="btn btn-success"><i class="fas fa-home"></i> チュートリアルTOPへ</a>
                    </div>
                </div>
            </main>
        </div>
    </div>
    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script>mermaid.initialize({ startOnLoad: true, theme: 'default' });</script>
</body>
</html>
