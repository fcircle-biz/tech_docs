<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spring AI + Ollama チュートリアル ステップ2 - Spring AIとOllamaの統合設定</title>

    <!-- Bootstrap 5 CDN -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Google Fonts - Noto Sans JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@300;400;500;700&display=swap" rel="stylesheet">

    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <!-- Highlight.js CDN -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

    <!-- Mermaid.js CDN -->
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>

    <style>
        :root {
            --primary-color: #f57c00;
            --secondary-color: #ff9800;
            --accent-color: #4caf50;
            --background-color: #fff3e0;
            --text-color: #333;
            --border-color: #ddd;
        }

        body {
            font-family: 'Noto Sans JP', sans-serif;
            background-color: #f8f9fa;
            padding-top: 56px;
        }

        .navbar {
            background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%);
        }

        .sidebar {
            position: fixed;
            top: 56px;
            bottom: 0;
            left: 0;
            z-index: 100;
            padding: 48px 0 0;
            box-shadow: inset -1px 0 0 rgba(0, 0, 0, .1);
            background-color: #f8f9fa;
            overflow-y: auto;
        }

        .sidebar-heading {
            font-size: .75rem;
            text-transform: uppercase;
        }

        .sidebar .nav-link {
            font-weight: 500;
            color: #333;
            padding: 0.5rem 1rem;
            border-left: 3px solid transparent;
        }

        .sidebar .nav-link:hover {
            color: var(--primary-color);
            background-color: var(--background-color);
            border-left-color: var(--primary-color);
        }

        .sidebar .nav-link.active {
            color: var(--primary-color);
            background-color: var(--background-color);
            border-left-color: var(--primary-color);
            font-weight: 700;
        }

        main {
            padding-top: 20px;
        }

        .chapter-title {
            color: var(--primary-color);
            border-bottom: 3px solid var(--primary-color);
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-weight: 700;
        }

        .section-title {
            color: var(--secondary-color);
            margin-top: 30px;
            margin-bottom: 20px;
            font-weight: 600;
            border-left: 4px solid var(--secondary-color);
            padding-left: 15px;
        }

        .highlight {
            background: linear-gradient(to right, #fff3e0, #ffe0b2);
            border-left: 4px solid var(--primary-color);
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .highlight h5 {
            color: var(--primary-color);
            font-weight: 700;
            margin-bottom: 15px;
        }

        .code-block {
            background-color: #282c34;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
        }

        .code-block code {
            color: #abb2bf;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .exercise-container {
            background-color: #f0f7ff;
            border: 2px solid #2196F3;
            border-radius: 8px;
            padding: 25px;
            margin: 25px 0;
        }

        .exercise-container h5 {
            color: #1976D2;
            font-weight: 700;
            margin-bottom: 15px;
        }

        .quiz-container {
            background-color: #f1f8e9;
            border: 2px solid var(--accent-color);
            border-radius: 8px;
            padding: 25px;
            margin: 25px 0;
        }

        .quiz-container h5 {
            color: var(--accent-color);
            font-weight: 700;
            margin-bottom: 15px;
        }

        .alert-custom {
            border-radius: 8px;
            padding: 15px 20px;
            margin: 15px 0;
        }

        footer {
            margin-top: 50px;
            padding: 30px 0;
            background-color: #212529;
        }

        .btn-primary {
            background-color: var(--primary-color);
            border-color: var(--primary-color);
        }

        .btn-primary:hover {
            background-color: #e65100;
            border-color: #e65100;
        }

        .mermaid {
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .concept-box {
            background-color: #e8f5e9;
            border: 2px solid #4caf50;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }

        .concept-box h6 {
            color: #2e7d32;
            font-weight: 700;
            margin-bottom: 10px;
        }
    </style>
</head>
<body>
    <!-- ナビゲーションバー -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="#">
                <i class="fab fa-java"></i>
                <strong>Spring AI + Ollama + Qwen3 実践チュートリアル</strong>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
    </nav>

    <div class="container-fluid">
        <div class="row">
            <!-- サイドバー -->
            <nav id="sidebarMenu" class="col-md-3 col-lg-2 d-md-block sidebar collapse">
                <div class="position-sticky pt-3">
                    <h6 class="sidebar-heading d-flex justify-content-between align-items-center px-3 mt-4 mb-1 text-muted">
                        <span>チュートリアルステップ</span>
                    </h6>
                    <ul class="nav flex-column">
                        <li class="nav-item">
                            <a class="nav-link" href="step1-environment-setup.html">
                                <i class="fas fa-cog"></i> ステップ1: 環境構築とプロジェクト作成
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link active" href="step2-spring-ai-ollama-integration.html">
                                <i class="fas fa-plug"></i> ステップ2: Spring AIとOllamaの統合設定
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="step3-chat-implementation.html">
                                <i class="fas fa-comments"></i> ステップ3: チャット機能の実装
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="step4-thymeleaf-view.html">
                                <i class="fas fa-file-code"></i> ステップ4: Thymeleafビューの作成
                            </a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="step5-testing.html">
                                <i class="fas fa-check-circle"></i> ステップ5: 動作確認
                            </a>
                        </li>
                    </ul>
                </div>
            </nav>

            <!-- メインコンテンツ -->
            <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
                <div class="d-flex justify-content-between flex-wrap flex-md-nowrap align-items-center pt-3 pb-2 mb-3 border-bottom">
                    <h1 class="h2"><i class="fas fa-plug"></i> ステップ2: Spring AIとOllamaの統合設定</h1>
                </div>

                <div id="step2">
                    <!-- ステップタイトル -->
                    <h2 class="chapter-title">Spring AIフレームワークの理解とOllama統合</h2>

                    <!-- このステップの目標 -->
                    <div class="highlight">
                        <h5><i class="fas fa-bullseye"></i> このステップで実装すること</h5>
                        <ul>
                            <li><strong>Spring AIの概要理解</strong> - 統一的なAI統合フレームワークの仕組み</li>
                            <li><strong>Ollamaの準備</strong> - ローカルLLM環境の起動とモデルダウンロード</li>
                            <li><strong>application.yml設定</strong> - Ollama接続パラメータの設定</li>
                            <li><strong>ChatClientの理解</strong> - Spring AIのコア機能を使用したAI対話</li>
                            <li><strong>プロンプトの基礎</strong> - システムプロンプトとユーザープロンプトの違い</li>
                            <li><strong>テストコントローラーの作成</strong> - 基本的なAI応答取得の実装</li>
                            <li><strong>エラーハンドリング</strong> - 接続エラーとタイムアウトの対処</li>
                        </ul>
                        <p><strong>所要時間：</strong> 約2.5時間</p>
                    </div>

                    <!-- 前提条件 -->
                    <div class="alert alert-info alert-custom">
                        <h6><i class="fas fa-info-circle"></i> 前提条件</h6>
                        <ul class="mb-0">
                            <li>ステップ1が完了し、Spring Bootアプリケーションが正常に起動すること</li>
                            <li>Docker Desktopが起動していること</li>
                            <li>最低4GBのメモリが利用可能なこと（Qwen3:1.7bモデル用）</li>
                        </ul>
                    </div>

                    <!-- セクション2.1: Spring AIフレームワークの概要 -->
                    <h3 class="section-title">2.1 Spring AIフレームワークの概要と特徴</h3>
                    <p>Spring AIは、Spring Bootアプリケーションに大規模言語モデル（LLM）を統合するための包括的なフレームワークです。</p>

                    <div class="concept-box">
                        <h6><i class="fas fa-lightbulb"></i> Spring AIの主要な特徴</h6>
                        <ul class="mb-0">
                            <li><strong>統一インターフェース</strong>: OpenAI、Ollama、Azure OpenAI、Google AI、Anthropic Claudeなど、複数のLLMプロバイダーを統一的に扱える</li>
                            <li><strong>Spring Boot自動設定</strong>: @SpringBootApplicationアノテーションで必要なBeanが自動的に設定される</li>
                            <li><strong>プロンプト管理</strong>: システムプロンプト、ユーザープロンプト、プロンプトテンプレートを明確に管理</li>
                            <li><strong>型安全</strong>: Javaの型システムによる安全なAI統合（コンパイル時エラー検出）</li>
                            <li><strong>RAG対応</strong>: Retrieval-Augmented Generation（検索拡張生成）をサポート</li>
                            <li><strong>ストリーミング</strong>: WebFluxと組み合わせてリアルタイムストリーミング応答を実装可能</li>
                        </ul>
                    </div>

                    <div class="mermaid">
                        flowchart TB
                            subgraph SpringBoot["Spring Bootアプリケーション"]
                                Controller["Controller/Service"]
                                ChatClient["ChatClient<br/>(Spring AI)"]
                            end

                            subgraph SpringAI["Spring AI統合レイヤー"]
                                Interface["統一インターフェース"]
                                Adapter["プロバイダー<br/>アダプター"]
                            end

                            subgraph Providers["LLMプロバイダー"]
                                Ollama["Ollama<br/>(ローカル)"]
                                OpenAI["OpenAI<br/>(クラウド)"]
                                Azure["Azure OpenAI<br/>(クラウド)"]
                            end

                            Controller --> ChatClient
                            ChatClient --> Interface
                            Interface --> Adapter
                            Adapter --> Ollama
                            Adapter -.-> OpenAI
                            Adapter -.-> Azure

                            style SpringBoot fill:#fff3e0
                            style SpringAI fill:#e8f5e9
                            style Ollama fill:#4caf50,color:#fff
                            style OpenAI fill:#10a37f,color:#fff
                            style Azure fill:#0078d4,color:#fff
                    </div>

                    <div class="concept-box">
                        <h6><i class="fas fa-question-circle"></i> なぜSpring AIを使うのか？</h6>
                        <p><strong>プロバイダー切り替えの容易さ:</strong> application.ymlの設定を変更するだけで、Ollamaから OpenAI、Azure OpenAIなどに切り替え可能</p>
                        <p><strong>Spring Boot統合:</strong> 既存のSpring Bootアプリケーションに最小限のコードでAI機能を追加できる</p>
                        <p><strong>エンタープライズ対応:</strong> Spring Securityとの統合、トランザクション管理、監視機能などが利用可能</p>
                    </div>

                    <!-- セクション2.2: Ollamaの起動とモデルダウンロード -->
                    <h3 class="section-title">2.2 Ollamaの起動とQwen3:1.7bモデルのダウンロード</h3>
                    <p>Ollamaは、ローカル環境でLLMを実行するためのツールです。Dockerコンテナで実行します。</p>

                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> 実装 2-1: OllamaコンテナとQwen3:1.7bモデルの準備</h5>

                        <h6><i class="fas fa-list-ol"></i> 手順</h6>
                        <ol>
                            <li><strong>Ollamaコンテナの起動</strong>:
                                <p>Docker Desktopが起動していることを確認してから、以下のコマンドを実行します：</p>
                                <pre class="code-block"><code class="language-bash"># Ollamaコンテナをバックグラウンドで起動
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  ollama/ollama:latest</code></pre>
                                <p><strong>パラメータ説明：</strong></p>
                                <ul>
                                    <li><code>-d</code>: デタッチモード（バックグラウンド実行）</li>
                                    <li><code>--name ollama</code>: コンテナ名を"ollama"に設定</li>
                                    <li><code>-p 11434:11434</code>: ポート11434を公開（Ollamaのデフォルトポート）</li>
                                    <li><code>-v ollama-data:/root/.ollama</code>: モデルデータの永続化</li>
                                </ul>
                            </li>

                            <li><strong>Qwen3:1.7bモデルのダウンロード</strong>:
                                <p>Ollamaコンテナ内でQwen3:1.7bモデルをダウンロードします（約1GB、2-5分かかります）：</p>
                                <pre class="code-block"><code class="language-bash"># Qwen3:1.7bモデルをpull
docker exec ollama ollama pull qwen3:1.7b</code></pre>
                            </li>

                            <li><strong>モデルの確認</strong>:
                                <p>ダウンロードが完了したら、利用可能なモデルを確認します：</p>
                                <pre class="code-block"><code class="language-bash">docker exec ollama ollama list</code></pre>
                            </li>
                        </ol>

                        <h6><i class="fas fa-check-circle"></i> 動作確認</h6>
                        <p>Ollamaが正常に動作しているか、簡単なテストを実行します：</p>
                        <pre class="code-block"><code class="language-bash"># Qwen3:1.7bとチャットしてみる
docker exec -it ollama ollama run qwen3:1.7b "日本の首都はどこですか？"</code></pre>

                        <h6><i class="fas fa-thumbs-up"></i> 期待される結果</h6>
                        <div class="alert alert-success">
                            <p><strong>ollama list の出力例:</strong></p>
                            <pre class="mb-3">NAME          ID              SIZE    MODIFIED
qwen3:1.7b    xxxxxxxxxxxx    1.0 GB  2 minutes ago</pre>

                            <p><strong>チャットテストの応答例:</strong></p>
                            <pre class="mb-0">日本の首都は東京（とうきょう）です。</pre>
                        </div>

                        <h6><i class="fas fa-exclamation-triangle"></i> トラブルシューティング</h6>
                        <div class="alert alert-warning">
                            <p><strong>エラー:</strong> docker: Error response from daemon: Conflict. The container name "/ollama" is already in use.</p>
                            <p><strong>解決方法:</strong> 既存のコンテナを削除してから再実行</p>
                            <pre class="code-block mb-3"><code class="language-bash">docker rm -f ollama
# その後、docker run コマンドを再実行</code></pre>

                            <p><strong>エラー:</strong> モデルのダウンロードが途中で止まる</p>
                            <p><strong>解決方法:</strong> ネットワーク接続を確認し、再度pullコマンドを実行（レジューム可能）</p>
                        </div>
                    </div>

                    <!-- セクション2.3: application.ymlでのOllama接続設定 -->
                    <h3 class="section-title">2.3 application.ymlでのOllama接続設定</h3>
                    <p>Spring AIがOllamaに接続できるよう、アプリケーション設定を行います。</p>

                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> 実装 2-2: application.ymlの設定</h5>

                        <h6><i class="fas fa-list-ol"></i> 手順</h6>
                        <ol>
                            <li><strong>application.ymlの編集</strong>:
                                <p>app/src/main/resources/application.ymlを開いて、以下の設定を追加します：</p>
                                <pre class="code-block"><code class="language-yaml">server:
  port: 8080

spring:
  application:
    name: ai-chat

  # Spring AI Ollama設定
  ai:
    ollama:
      base-url: http://localhost:11434  # OllamaサーバーのURL
      chat:
        model: qwen3:1.7b                # 使用するモデル
        options:
          temperature: 0.7               # 生成の多様性（0.0-1.0）
          top-p: 0.9                     # トークン選択の閾値
          num-predict: 2000              # 最大生成トークン数</code></pre>
                            </li>

                            <li><strong>設定パラメータの説明</strong>:
                                <div class="table-responsive">
                                    <table class="table table-bordered">
                                        <thead class="table-light">
                                            <tr>
                                                <th>パラメータ</th>
                                                <th>説明</th>
                                                <th>推奨値</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td><code>base-url</code></td>
                                                <td>OllamaサーバーのエンドポイントURL</td>
                                                <td>http://localhost:11434</td>
                                            </tr>
                                            <tr>
                                                <td><code>model</code></td>
                                                <td>使用するLLMモデル名</td>
                                                <td>qwen3:1.7b</td>
                                            </tr>
                                            <tr>
                                                <td><code>temperature</code></td>
                                                <td>生成の多様性・ランダム性（0.0=決定的、1.0=創造的）</td>
                                                <td>0.7（バランス型）</td>
                                            </tr>
                                            <tr>
                                                <td><code>top-p</code></td>
                                                <td>累積確率閾値（nucleus sampling）</td>
                                                <td>0.9</td>
                                            </tr>
                                            <tr>
                                                <td><code>num-predict</code></td>
                                                <td>最大生成トークン数（長い応答用）</td>
                                                <td>2000</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                            </li>
                        </ol>

                        <div class="concept-box">
                            <h6><i class="fas fa-lightbulb"></i> temperatureパラメータの使い分け</h6>
                            <ul class="mb-0">
                                <li><strong>0.0-0.3:</strong> 決定的・事実に基づく回答が必要な場合（FAQ、ドキュメント検索）</li>
                                <li><strong>0.4-0.7:</strong> バランス型。一般的な対話に最適</li>
                                <li><strong>0.8-1.0:</strong> 創造的な生成が必要な場合（小説執筆、アイデア出し）</li>
                            </ul>
                        </div>

                        <div class="concept-box">
                            <h6><i class="fas fa-lightbulb"></i> top-pパラメータの使い分け</h6>
                            <ul class="mb-0">
                                <li><strong>0.3-0.5:</strong> 上位の限られた語から選択 → 安定・論理的（事実確認、技術文書生成）</li>
                                <li><strong>0.8-0.9:</strong> 上位の確率語を広めに選ぶ → 自然で多様性あり（一般的な対話）</li>
                                <li><strong>1.0:</strong> ほぼ制約なし → 非常にランダムだが、誤答のリスクも増（創造的な文章生成）</li>
                            </ul>
                        </div>
                    </div>

                    <!-- セクション2.4: ChatClientの理解と実装 -->
                    <h3 class="section-title">2.4 ChatClientインターフェースを使用したAI対話</h3>
                    <p>Spring AIのChatClientは、LLMとの対話を簡潔に実装できるインターフェースです。</p>

                    <div class="concept-box">
                        <h6><i class="fas fa-code"></i> ChatClientの基本構造</h6>
                        <pre class="code-block"><code class="language-java">// ChatClient.Builderを使用したチェーンメソッド
String response = chatClient.prompt()
    .system("あなたは親切なAIアシスタントです")  // システムプロンプト
    .user("日本の首都はどこですか？")           // ユーザープロンプト
    .call()                                     // API呼び出し
    .content();                                 // レスポンスコンテンツ取得</code></pre>
                    </div>

                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> 実装 2-3: テストコントローラーでChatClientを使用</h5>

                        <h6><i class="fas fa-list-ol"></i> 手順</h6>
                        <ol>
                            <li><strong>ChatClient.Builderの注入設定</strong>:
                                <p>Spring AIは自動的にChatClient.BuilderをBeanとして登録します。これを利用してテストコントローラーを作成します。</p>
                            </li>

                            <li><strong>TestAiControllerの作成</strong>:
                                <p>app/src/main/java/com/example/aichat/TestAiController.javaを作成：</p>
                                <pre class="code-block"><code class="language-java">package com.example.aichat;

import org.springframework.ai.chat.client.ChatClient;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class TestAiController {

    private final ChatClient chatClient;

    // コンストラクタインジェクション（推奨）
    public TestAiController(ChatClient.Builder chatClientBuilder) {
        this.chatClient = chatClientBuilder.build();
    }

    /**
     * AIとの基本的な対話テスト
     * 使用例: http://localhost:8080/ai/test?message=日本の首都はどこですか？
     */
    @GetMapping("/ai/test")
    public String testAi(@RequestParam String message) {
        try {
            String response = chatClient.prompt()
                .user(message)
                .call()
                .content();

            return response;
        } catch (Exception e) {
            return "エラーが発生しました: " + e.getMessage();
        }
    }

    /**
     * システムプロンプトを使用した対話テスト
     * 使用例: http://localhost:8080/ai/test-system?message=こんにちは
     */
    @GetMapping("/ai/test-system")
    public String testWithSystemPrompt(@RequestParam String message) {
        try {
            String response = chatClient.prompt()
                .system("あなたは親切で丁寧な日本語AIアシスタントです。" +
                        "常に敬語を使い、簡潔で分かりやすい回答を心がけてください。")
                .user(message)
                .call()
                .content();

            return response;
        } catch (Exception e) {
            return "エラーが発生しました: " + e.getMessage();
        }
    }
}</code></pre>
                            </li>

                            <li><strong>HelloControllerの削除（オプション）</strong>:
                                <p>ステップ1で作成したHelloController.javaは不要になったため削除できます（残しておいても問題ありません）。</p>
                            </li>
                        </ol>

                        <h6><i class="fas fa-check-circle"></i> 動作確認</h6>
                        <p>アプリケーションを起動して、ブラウザまたはcurlでテストします：</p>
                        <pre class="code-block"><code class="language-bash"># Windows (コマンドプロンプト)
cd app
gradlew bootRun

# Windows (PowerShell)
cd app
.\gradlew bootRun

# macOS / Linux
cd app
./gradlew bootRun</code></pre>

                        <p><strong>Windows環境のブラウザでテスト:</strong></p>
                        <pre class="code-block"><code># テスト1: 基本的なAI対話
http://localhost:8080/ai/test?message=日本の首都はどこですか？

# テスト2: システムプロンプト付き対話
http://localhost:8080/ai/test-system?message=こんにちは</code></pre>

                        <p><strong>Linux環境でcurlを使用:</strong></p>
                        <pre class="code-block"><code class="language-bash"># テスト1: 基本的なAI対話
curl "http://localhost:8080/ai/test?message=日本の首都はどこですか？"

# テスト2: システムプロンプト付き対話
curl "http://localhost:8080/ai/test-system?message=こんにちは"</code></pre>

                        <h6><i class="fas fa-thumbs-up"></i> 期待される結果</h6>
                        <div class="alert alert-success">
                            <p><strong>基本的なAI対話の応答例:</strong></p>
                            <pre class="mb-3">日本の首都は東京（とうきょう）です。</pre>

                            <p><strong>システムプロンプト付き対話の応答例:</strong></p>
                            <pre class="mb-0">こんにちは！本日はどのようなご質問やお困りごとがございますか？お気軽にお聞かせください。</pre>
                        </div>

                        <h6><i class="fas fa-exclamation-triangle"></i> トラブルシューティング</h6>
                        <div class="alert alert-warning">
                            <p><strong>エラー:</strong> Connection refused: localhost/127.0.0.1:11434</p>
                            <p><strong>解決方法:</strong></p>
                            <ul>
                                <li>Ollamaコンテナが起動しているか確認: <code>docker ps</code></li>
                                <li>起動していない場合: <code>docker start ollama</code></li>
                                <li>ポート11434が使用可能か確認</li>
                            </ul>

                            <p><strong>エラー:</strong> Model 'qwen3:1.7b' not found</p>
                            <p><strong>解決方法:</strong></p>
                            <ul class="mb-0">
                                <li>モデルがダウンロードされているか確認: <code>docker exec ollama ollama list</code></li>
                                <li>ダウンロードされていない場合: <code>docker exec ollama ollama pull qwen3:1.7b</code></li>
                            </ul>
                        </div>
                    </div>

                    <!-- セクション2.5: システムプロンプトとユーザープロンプト -->
                    <h3 class="section-title">2.5 プロンプトエンジニアリングの基礎</h3>
                    <p>効果的なAI対話を実現するため、システムプロンプトとユーザープロンプトの違いを理解します。</p>

                    <div class="concept-box">
                        <h6><i class="fas fa-brain"></i> システムプロンプトとユーザープロンプトの違い</h6>
                        <table class="table table-bordered mt-3">
                            <thead class="table-light">
                                <tr>
                                    <th style="width: 30%;">種類</th>
                                    <th>役割</th>
                                    <th>例</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td><strong>システムプロンプト</strong><br><code>.system()</code></td>
                                    <td>
                                        <ul class="mb-0">
                                            <li>AIの振る舞いや役割を定義</li>
                                            <li>応答の形式やスタイルを指定</li>
                                            <li>制約や制限事項を設定</li>
                                            <li>会話全体に影響する</li>
                                        </ul>
                                    </td>
                                    <td>「あなたは専門的なJavaプログラミングアシスタントです。コード例を含めて回答してください。」</td>
                                </tr>
                                <tr>
                                    <td><strong>ユーザープロンプト</strong><br><code>.user()</code></td>
                                    <td>
                                        <ul class="mb-0">
                                            <li>ユーザーからの具体的な質問や依頼</li>
                                            <li>対話ごとに変化する</li>
                                            <li>システムプロンプトの制約下で処理される</li>
                                        </ul>
                                    </td>
                                    <td>「Spring Bootでデータベース接続する方法を教えてください」</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>

                    <div class="exercise-container">
                        <h5><i class="fas fa-tasks"></i> 実習 2-4: 効果的なシステムプロンプトの作成</h5>
                        <p>システムプロンプトの設計によって、AIの応答品質が大きく変わります。以下の例で試してみましょう：</p>

                        <h6>例1: 技術的な質問応答アシスタント</h6>
                        <pre class="code-block"><code class="language-java">String response = chatClient.prompt()
    .system("""
        あなたは経験豊富なJavaエンジニアです。
        以下のルールに従って回答してください：
        1. 技術的に正確で最新の情報を提供する
        2. 可能な限りコード例を含める
        3. セキュリティのベストプラクティスを考慮する
        4. 簡潔で分かりやすく説明する
        """)
    .user("Spring Securityの基本設定を教えてください")
    .call()
    .content();</code></pre>

                        <h6>例2: 教育的なアシスタント</h6>
                        <pre class="code-block"><code class="language-java">String response = chatClient.prompt()
    .system("""
        あなたは初心者向けのプログラミング講師です。
        以下のスタイルで回答してください：
        1. 専門用語を使う場合は必ず説明を加える
        2. 段階的に説明する
        3. 具体的な例を示す
        4. 励ましの言葉を添える
        """)
    .user("変数とは何ですか？")
    .call()
    .content();</code></pre>

                        <h6>例3: JSON形式で応答するアシスタント</h6>
                        <pre class="code-block"><code class="language-java">String response = chatClient.prompt()
    .system("""
        あなたは構造化データを生成するAPIです。
        回答は必ず以下のJSON形式で返してください：
        {
          "answer": "回答内容",
          "confidence": 0.0-1.0の数値,
          "sources": ["情報源1", "情報源2"]
        }
        """)
    .user("Spring Bootの最新バージョンは？")
    .call()
    .content();</code></pre>
                    </div>

                    <!-- Spring AI アーキテクチャ図 -->
                    <h3 class="section-title">2.6 Spring AI統合アーキテクチャ</h3>
                    <p>このステップで実装した構成の全体像を確認します。</p>

                    <div class="mermaid">
                        sequenceDiagram
                            autonumber
                            participant Browser as ブラウザ
                            participant Controller as TestAiController
                            participant ChatClient as ChatClient
                            participant SpringAI as Spring AI
                            participant Ollama as Ollamaコンテナ
                            participant Model as Qwen3:1.7bモデル

                            Browser->>Controller: GET /ai/test?message=質問
                            Controller->>ChatClient: prompt().user("質問")
                            ChatClient->>SpringAI: プロンプト構築
                            SpringAI->>Ollama: HTTP POST /api/chat
                            Ollama->>Model: 推論実行
                            Model-->>Ollama: 生成テキスト
                            Ollama-->>SpringAI: JSON応答
                            SpringAI-->>ChatClient: Javaオブジェクト
                            ChatClient-->>Controller: String content
                            Controller-->>Browser: AI応答テキスト
                    </div>

                    <!-- ステップ完了チェック -->
                    <div class="quiz-container">
                        <h5><i class="fas fa-clipboard-check"></i> ステップ2完了チェックリスト</h5>
                        <p>以下の項目が完了していることを確認してください：</p>
                        <ul class="list-unstyled">
                            <li><input type="checkbox"> Spring AIフレームワークの概要を理解している</li>
                            <li><input type="checkbox"> Ollamaコンテナが起動している（<code>docker ps</code>で確認）</li>
                            <li><input type="checkbox"> Qwen3:1.7bモデルがダウンロードされている（<code>docker exec ollama ollama list</code>で確認）</li>
                            <li><input type="checkbox"> application.ymlにOllama接続設定が追加されている</li>
                            <li><input type="checkbox"> TestAiControllerが作成されている</li>
                            <li><input type="checkbox"> <code>http://localhost:8080/ai/test?message=こんにちは</code> でAI応答が取得できる</li>
                            <li><input type="checkbox"> システムプロンプトとユーザープロンプトの違いを理解している</li>
                            <li><input type="checkbox"> ChatClient.Builderの使い方を理解している</li>
                        </ul>
                    </div>

                    <!-- 現在のファイル構成 -->
                    <div class="alert alert-light alert-custom">
                        <h6><i class="fas fa-folder-tree"></i> 現在のプロジェクト構成</h6>
                        <pre class="code-block"><code class="language-bash">ai-chat/
├── app/
│   ├── build.gradle
│   └── src/
│       ├── main/
│       │   ├── java/com/example/aichat/
│       │   │   ├── AiChatApplication.java      # メインクラス
│       │   │   └── TestAiController.java        # [新規] AIテストコントローラー
│       │   └── resources/
│       │       └── application.yml              # [更新] Ollama接続設定追加
│       └── test/
├── gradlew
└── settings.gradle

# Docker環境
ollama コンテナ (ポート11434)
└── qwen3:1.7b モデル (約1GB)</code></pre>
                    </div>

                    <!-- 次のステップ -->
                    <div class="alert alert-info alert-custom">
                        <h6><i class="fas fa-forward"></i> 次のステップの予告</h6>
                        <p><strong>ステップ3: チャット機能の実装</strong>では、以下の内容を実装します：</p>
                        <ul class="mb-0">
                            <li>ChatControllerの作成（@Controller）</li>
                            <li>GETリクエストハンドラ（初期画面表示）</li>
                            <li>POSTリクエストハンドラ（チャット送信処理）</li>
                            <li>ChatFormオブジェクトとバリデーション（@NotBlank）</li>
                            <li>プロンプトエンジニアリングの実践</li>
                        </ul>
                    </div>

                    <!-- ステップ間ナビゲーション -->
                    <div class="d-flex justify-content-between mt-5 mb-4">
                        <a href="step1-environment-setup.html" class="btn btn-secondary">
                            <i class="fas fa-arrow-left"></i> 前のステップ：環境構築
                        </a>
                        <a href="step3-chat-implementation.html" class="btn btn-primary">
                            次のステップ：チャット機能実装 <i class="fas fa-arrow-right"></i>
                        </a>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <!-- フッター -->
    <footer class="bg-dark text-white mt-5">
        <div class="container-fluid py-3">
            <div class="row">
                <div class="col-12 text-center">
                    <p class="mb-0">© 2025 F-Circle. All rights reserved.<br>
本資料はAIツールを活用し、人間による編集・監修のもと作成されています。無断転載・再配布を禁じます。</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- Bootstrap JavaScript -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>

    <!-- Highlight.js 初期化 -->
    <script>hljs.highlightAll();</script>

    <!-- Mermaid.js 初期化 -->
    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'default'
        });
    </script>
</body>
</html>
